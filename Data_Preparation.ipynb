{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c4c1845-dbd8-4b23-8520-e0e57905d6e1",
   "metadata": {},
   "source": [
    "## Importing Required Python Libraries\n",
    "\n",
    "This section is dedicated to importing all the necessary Python libraries that are essential for executing the subsequent parts of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1855436-114b-4c68-a63b-b74e7ebd6247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Required Python Libraries\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0469b6b0-3225-421a-a50f-bfc31cf0c4da",
   "metadata": {},
   "source": [
    "## Phase 1: Data Collection\n",
    "\n",
    "In this initial phase, we gather historical stock data for a specified list of ticker symbols within a defined date range. Utilizing the **Collect_Stock_Data** function, we iterate through each ticker symbol, retrieving its historical market data using the **yfinance** library. Subsequently, the data is stored in a dictionary after ensuring its validity and sufficiency. Feedback messages are provided to indicate the success or failure of data collection for each ticker symbol.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "482fa21e-a411-4288-b3e4-f59d5346c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Collect_Stock_Data(start_date, end_date, list_of_ticker_symbols):\n",
    "\n",
    "    \"\"\"\n",
    "    Collects historical stock data for a given list of ticker symbols within a specified date range.\n",
    "\n",
    "    This function iterates through each ticker symbol, retrieves its historical market data using\n",
    "    the yfinance library, and stores the data in a dictionary if it meets certain conditions. It checks\n",
    "    whether the data is not empty and if the number of rows meets at least 80% of the expected number of\n",
    "    days within the given timeframe, assuming 250 trading days in a year.\n",
    "\n",
    "    Parameters:\n",
    "    - start_date (str): The start date for the data collection in \"YYYY-MM-DD\" format.\n",
    "    - end_date (str): The end date for the data collection in \"YYYY-MM-DD\" format.\n",
    "    - list_of_ticker_symbols (list): A list of string ticker symbols for which to collect data.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary with ticker symbols as keys and their corresponding historical data DataFrames as values.\n",
    "\n",
    "    Note:\n",
    "    The function prints messages indicating the success of data collection for each ticker or the failure\n",
    "    to meet the data requirements.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dictionary to store the historical data DataFrames\n",
    "    dataframes= {}\n",
    "    \n",
    "    # Extracting Stock Data\n",
    "    for stock in list_of_ticker_symbols:        \n",
    "\n",
    "        stk = yf.Ticker(stock)\n",
    "        \n",
    "        current_data = stk.history(start=start_date, end=end_date)\n",
    "        \n",
    "        # Calculate the number of rows and check against the 80% threshold of expected days\n",
    "        number_rows = current_data.shape[0]\n",
    "        expected_days = (250/365)*((datetime.strptime(end_date, \"%Y-%m-%d\") - datetime.strptime(start_date, \"%Y-%m-%d\")).days)*0.8\n",
    "        \n",
    "        if (current_data.empty==False) and (number_rows>=expected_days):\n",
    "            print(stock, 'data has been collected successfully.')\n",
    "            # Store the DataFrame\n",
    "            dataframes[stock] = current_data\n",
    "        else:\n",
    "            print('Sorry we can not do prediction for {}, Since No data found for this stock in this time frame.'.format(stock))\n",
    "            \n",
    "    return dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50e19957-111a-4362-8f42-d3025b7af9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_ticker_symbols = ['MSFT', 'UPS']\n",
    "start_date = '2013-11-10'\n",
    "end_date = '2023-11-10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15fe2760-8956-4905-a0f9-3e84435eb908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSFT data has been collected successfully.\n",
      "UPS data has been collected successfully.\n"
     ]
    }
   ],
   "source": [
    "dataframes = Collect_Stock_Data(start_date, end_date, list_of_ticker_symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2756068a-eb99-4f62-9776-15227135d81d",
   "metadata": {},
   "source": [
    "## Phase 2: Data Cleaning - Handling Missing Values\n",
    "\r\n",
    "In this phase, continuous NaN (Not a Number) sequences within the dataset are addressed through data cleaning. Rows that are part of a continuous sequence of NaNs exceeding a predefined threshold are identified and removed by the Drop_Continuous_NaNs function. The balance between data retention and the elimination of potentially problematic missing data sequences is achieved by specifying the maximum allowed sequence length of NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41120320-e3ac-47e0-982c-573b0ee49981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-11-11 00:00:00-05:00</th>\n",
       "      <td>31.440089</td>\n",
       "      <td>31.515165</td>\n",
       "      <td>31.156469</td>\n",
       "      <td>31.356672</td>\n",
       "      <td>26872500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-11-12 00:00:00-05:00</th>\n",
       "      <td>31.181494</td>\n",
       "      <td>31.365011</td>\n",
       "      <td>31.031342</td>\n",
       "      <td>31.164810</td>\n",
       "      <td>31651600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-11-13 00:00:00-05:00</th>\n",
       "      <td>30.847814</td>\n",
       "      <td>31.832142</td>\n",
       "      <td>30.781082</td>\n",
       "      <td>31.832142</td>\n",
       "      <td>44957600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-11-14 00:00:00-05:00</th>\n",
       "      <td>31.590239</td>\n",
       "      <td>31.807126</td>\n",
       "      <td>31.465114</td>\n",
       "      <td>31.715366</td>\n",
       "      <td>46183700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-11-15 00:00:00-05:00</th>\n",
       "      <td>31.656982</td>\n",
       "      <td>31.715374</td>\n",
       "      <td>31.465122</td>\n",
       "      <td>31.565222</td>\n",
       "      <td>50601300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-03 00:00:00-04:00</th>\n",
       "      <td>348.277763</td>\n",
       "      <td>353.019363</td>\n",
       "      <td>345.986641</td>\n",
       "      <td>351.435486</td>\n",
       "      <td>23624000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-06 00:00:00-05:00</th>\n",
       "      <td>352.083018</td>\n",
       "      <td>356.157196</td>\n",
       "      <td>351.983399</td>\n",
       "      <td>355.151093</td>\n",
       "      <td>23828300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-07 00:00:00-05:00</th>\n",
       "      <td>358.009986</td>\n",
       "      <td>361.058148</td>\n",
       "      <td>356.246842</td>\n",
       "      <td>359.135620</td>\n",
       "      <td>25833900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-08 00:00:00-05:00</th>\n",
       "      <td>360.281147</td>\n",
       "      <td>362.462680</td>\n",
       "      <td>359.155513</td>\n",
       "      <td>361.795288</td>\n",
       "      <td>26767800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-09 00:00:00-05:00</th>\n",
       "      <td>360.898741</td>\n",
       "      <td>363.379132</td>\n",
       "      <td>358.966242</td>\n",
       "      <td>359.294983</td>\n",
       "      <td>24847300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2517 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Open        High         Low       Close  \\\n",
       "Date                                                                        \n",
       "2013-11-11 00:00:00-05:00   31.440089   31.515165   31.156469   31.356672   \n",
       "2013-11-12 00:00:00-05:00   31.181494   31.365011   31.031342   31.164810   \n",
       "2013-11-13 00:00:00-05:00   30.847814   31.832142   30.781082   31.832142   \n",
       "2013-11-14 00:00:00-05:00   31.590239   31.807126   31.465114   31.715366   \n",
       "2013-11-15 00:00:00-05:00   31.656982   31.715374   31.465122   31.565222   \n",
       "...                               ...         ...         ...         ...   \n",
       "2023-11-03 00:00:00-04:00  348.277763  353.019363  345.986641  351.435486   \n",
       "2023-11-06 00:00:00-05:00  352.083018  356.157196  351.983399  355.151093   \n",
       "2023-11-07 00:00:00-05:00  358.009986  361.058148  356.246842  359.135620   \n",
       "2023-11-08 00:00:00-05:00  360.281147  362.462680  359.155513  361.795288   \n",
       "2023-11-09 00:00:00-05:00  360.898741  363.379132  358.966242  359.294983   \n",
       "\n",
       "                             Volume  Dividends  Stock Splits  \n",
       "Date                                                          \n",
       "2013-11-11 00:00:00-05:00  26872500        0.0           0.0  \n",
       "2013-11-12 00:00:00-05:00  31651600        0.0           0.0  \n",
       "2013-11-13 00:00:00-05:00  44957600        0.0           0.0  \n",
       "2013-11-14 00:00:00-05:00  46183700        0.0           0.0  \n",
       "2013-11-15 00:00:00-05:00  50601300        0.0           0.0  \n",
       "...                             ...        ...           ...  \n",
       "2023-11-03 00:00:00-04:00  23624000        0.0           0.0  \n",
       "2023-11-06 00:00:00-05:00  23828300        0.0           0.0  \n",
       "2023-11-07 00:00:00-05:00  25833900        0.0           0.0  \n",
       "2023-11-08 00:00:00-05:00  26767800        0.0           0.0  \n",
       "2023-11-09 00:00:00-05:00  24847300        0.0           0.0  \n",
       "\n",
       "[2517 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes['MSFT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54528d84-0dcf-4842-9fc6-0fc5be97a51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Drop_Continuous_NaNs(df, max_allowed_continuous_nans=3):\n",
    "    \n",
    "    \"\"\"\n",
    "    Filters out rows from a pandas DataFrame that are part of a continuous sequence of NaNs (Not a Number)\n",
    "    exceeding a specified threshold in any column.\n",
    "\n",
    "    The function identifies sequences of continuous NaNs across all columns and drops any rows that are\n",
    "    part of a sequence longer than `max_allowed_continuous_nans`. It handles each column independently,\n",
    "    ensuring that no column has continuous NaNs exceeding the threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The DataFrame to be processed.\n",
    "    - max_allowed_continuous_nans (int, optional): The maximum allowed length of continuous NaN sequences.\n",
    "      Rows that are part of a sequence longer than this will be dropped. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: A DataFrame with rows in continuous NaN sequences beyond the allowed threshold removed.\n",
    "\n",
    "    Note:\n",
    "    The function directly analyzes the NaN presence in each row, groups continuous NaN sequences,\n",
    "    and filters based on the specified threshold. It preserves the original DataFrame structure\n",
    "    and data integrity, except for the removal of specified sequences.\n",
    "    \"\"\"\n",
    "\n",
    "    # Identify rows that are completely NaN\n",
    "    all_nan_rows = df.isna().all(axis=1)\n",
    "    \n",
    "    # Group consecutive NaN rows to identify continuous sequences\n",
    "    all_nan_groups = (~all_nan_rows).cumsum()\n",
    "    \n",
    "    # Calculate the size of each group of continuous NaN rows\n",
    "    group_sizes = all_nan_groups.map(all_nan_groups.value_counts())\n",
    "    \n",
    "    # Generate a mask for valid rows: either not part of a NaN sequence or within the allowed NaN sequence length\n",
    "    valid_rows_mask = (group_sizes <= max_allowed_continuous_nans) | (~all_nan_rows)\n",
    "    \n",
    "    # Apply the mask to filter the DataFrame, removing unwanted continuous NaN sequences\n",
    "    df_filtered = df[valid_rows_mask]\n",
    "    \n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f764e2-0bf7-4f6b-b517-2936e2295ecd",
   "metadata": {},
   "source": [
    "## Phase 3: Data Preprocessing - NaN Values Cleaning\n",
    "\r\n",
    "In this phase, NaN values across multiple pandas DataFrames are cleaned to ensure data quality for analysis. The Clean_NaN_Values function is employed to methodically process each DataFrame contained within a provided dictionary. Initially, rows featuring continuous sequences of NaN values exceeding a predefined threshold are identified and removed. Subsequently, any remaining NaNs within specific columns are addressed through deletion, filling, or imputation, contingent upon the nature of the data and the proportion of missing valuesis.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82c8aa97-460c-4918-8563-fbaf14bc62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clean_NaN_Values(dataframes):\n",
    "    \n",
    "    \"\"\"\n",
    "    Cleans NaN values across a collection of pandas DataFrames by first removing rows with continuous\n",
    "    NaNs beyond a set threshold, then addressing remaining NaNs in specific columns through either deletion,\n",
    "    filling, or imputation based on the nature of the data and the proportion of missing values.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframes (dict): A dictionary where keys are identifiers (e.g., ticker symbols) and values are\n",
    "      pandas DataFrames containing stock market data.\n",
    "\n",
    "    Returns:\n",
    "    - dict: The same dictionary with DataFrames cleaned of NaN values according to the specified rules.\n",
    "\n",
    "    Note:\n",
    "    This function specifically treats financial data columns differently based on their importance and\n",
    "    the feasibility of imputation. It also provides warnings for DataFrames where critical columns like\n",
    "    'Close' have a high percentage of NaN values, potentially indicating inadequate data for reliable analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    for ticker in dataframes:\n",
    "        df = dataframes[ticker]\n",
    "\n",
    "        # Drop rows with continuous NaNs longer than the threshold\n",
    "        df = Drop_Continuous_NaNs(df, max_allowed_continuous_nans=3)    \n",
    "\n",
    "        # Calculate the percentage of NaNs in the 'Close' column\n",
    "        closing_price_NaN = df['Close'].isna().mean() * 100\n",
    "        NaN_threshold_for_close = 12\n",
    "\n",
    "        # Check if 'Close' column NaN percentage exceeds threshold\n",
    "        if closing_price_NaN > NaN_threshold_for_close:\n",
    "            print('Sorry we can not predict {} since more than {} of the Close Price column is NaN'.format(ticker, NaN_threshold_for_close))\n",
    "            \n",
    "        else:\n",
    "            for column in df.columns:\n",
    "                # Fill NaNs with 0 for 'Dividends' and 'Stock Splits'\n",
    "                if column in ['Dividends', 'Stock Splits']:\n",
    "                    df[column].fillna(0, inplace=True)\n",
    "\n",
    "                # Analyze and handle NaNs for financial columns differently\n",
    "                if column in ['Open', 'High', 'Low', 'Close', 'Volume']:\n",
    "                    # NaN analysis on the cleaned DataFrame\n",
    "                    nan_percent = df[column].isna().mean() * 100\n",
    "\n",
    "                    # If >50% NaNs, drop the column with a warning\n",
    "                    if nan_percent > 50:\n",
    "                        \n",
    "                            # Attempt to acquire more complete data or use domain knowledge\n",
    "                            print(f\"Column: {column} has >50% NaNs. Dropping this column. Consider acquiring more complete data.\")\n",
    "                            del df[column]\n",
    "                    # For NaN percentages between 0 and 50\n",
    "                    elif 0 < nan_percent <= 50:\n",
    "                        # Use forward fill and backward fill for low NaN percentage\n",
    "                        if nan_percent < 5: \n",
    "                            # Forward fill for financial time series data\n",
    "                            df[column]= df[column].fillna(method='ffill').fillna(method='bfill')\n",
    "                            \n",
    "                        else:\n",
    "                            # Apply KNN imputation for higher NaN percentages to the entire DataFrame (considering all columns)                            \n",
    "                            columns_for_imputation = df[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "                            knn_imputer = KNNImputer(n_neighbors=5)\n",
    "                            imputed_data = knn_imputer.fit_transform(columns_for_imputation)\n",
    "                            imputed_df = pd.DataFrame(imputed_data, columns=columns_for_imputation.columns)\n",
    "                            \n",
    "                            # Merge the imputed 'Volume' column back into the original DataFrame (Step 5)\n",
    "                            df[column] = imputed_df[column]\n",
    "                    \n",
    "                    else:\n",
    "                        # No action needed for columns without NaN values\n",
    "                        pass\n",
    "        # Update the DataFrame in the dictionary\n",
    "        dataframes[ticker] = df\n",
    "\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecfc96df-b22d-4c91-8d55-331d667bd21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = Clean_NaN_Values(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580cddbd-f3e6-43da-9502-fc09690fda0f",
   "metadata": {},
   "source": [
    "## Phase 4: Data Enhancement - Adjusting Stock Prices\n",
    "\n",
    "This stage is dedicated to refining the dataset by calculating adjusted closing prices for stocks, encapsulating the effect of corporate actions like stock splits and dividends. The Calculate_Adjusted_Price function updates the 'Adjusted_Close' column within each DataFrame from a collection keyed by ticker symbols. Adjustments ensure that the historical prices reflect true stock value post-corporate events, providing a more accurate basis for financial analysis.\n",
    "\n",
    "Adjustments are applied in two folds: prices are recalibrated for stock splits by direct modification, and for dividends, by adjusting the price to mirror the payout’s impact on stock value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6a6854f-f353-45a4-beba-8f4c9773e4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_Adjusted_Price(dataframes):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculates the adjusted closing price for stocks within each DataFrame in a collection, taking into account\n",
    "    stock splits and dividends. The adjusted closing price reflects the stock's value after accounting for\n",
    "    corporate actions.\n",
    "\n",
    "    Adjustments are made by modifying the 'Adjusted_Close' column based on the 'Stock Splits' and 'Dividends'\n",
    "    columns. Stock splits adjust the price directly, while dividends adjust the price to reflect the payout's\n",
    "    impact on the stock's value.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframes (dict): A dictionary of pandas DataFrames keyed by ticker symbols \n",
    "\n",
    "    Returns:\n",
    "    - dict: The same dictionary with each DataFrame's 'Adjusted_Close' column updated to reflect the adjusted\n",
    "      closing prices.\n",
    "\n",
    "    Note:\n",
    "    The function processes each DataFrame independently, ensuring the date column is in the correct format\n",
    "    and sorted in ascending order before making adjustments.\n",
    "    \"\"\"\n",
    "    \n",
    "    for ticker in dataframes:\n",
    "        df = dataframes[ticker]\n",
    "        \n",
    "        # Ensure the 'Date' column is in 'YYYY-MM-DD' format and set as datetime type\n",
    "        df.reset_index(inplace= True)\n",
    "        df['Date'] = df['Date'].apply(lambda x: str(x).split(' ')[0])\n",
    "        df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
    "            \n",
    "        df.sort_values('Date', ascending=True, inplace=True)\n",
    "        \n",
    "        # Initialize 'Adjusted_Close' column with existing close prices\n",
    "        df['Adjusted_Close'] = df['Close']\n",
    "        \n",
    "        # Adjust 'Adjusted_Close' for stock splits\n",
    "        for i in df[df['Stock Splits'] != 0].index:\n",
    "            split_factor = df.at[i, 'Stock Splits']\n",
    "            df.loc[:i-1, 'Adjusted_Close'] /= split_factor\n",
    "        \n",
    "        # Adjust 'Adjusted_Close' for dividends\n",
    "        for i in df[df['Dividends'] != 0].index:\n",
    "            dividend = df.at[i, 'Dividends']\n",
    "            price = df.at[i-1, 'Close']\n",
    "            Dividend_Multiplier = 1 - (dividend/price)\n",
    "            df.loc[:i-1, 'Adjusted_Close'] *= Dividend_Multiplier\n",
    "            \n",
    "        dataframes[ticker] = df\n",
    "        \n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2edb005-c552-48be-a551-e1b6f283efd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = Calculate_Adjusted_Price(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708649b0-b37b-4974-8ffc-70a233b3b42a",
   "metadata": {},
   "source": [
    "## Phase 5: Momentum Analysis - Calculating RSI\n",
    "\n",
    "In this phase, the dataset undergoes enhancement through the calculation of the Relative Strength Index (RSI) for stocks, a pivotal momentum indicator. The calculate_rsi function is integral to this process, applying the RSI calculation across specified columns in each DataFrame within the collection.\n",
    "\n",
    "This function, by utilizing a typical 14-day period for evaluation, enables an assessment of stock momentum by segregating price movements into gains and losses. These segregated movements are then averaged and utilized to compute the RSI, enriching each DataFrame with a new 'RSI' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c169e0b-014a-47d7-879f-a66191e54f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_RSI(dataframes, column='Adjusted_Close', period=14):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates the Relative Strength Index (RSI) for the specified column in each DataFrame within a\n",
    "    collection. RSI is a momentum indicator that measures the magnitude of recent price changes to evaluate\n",
    "    overbought or oversold conditions in the price of a stock.\n",
    "\n",
    "    The RSI is calculated using a specified period (typically 14 days) and is scaled from 0 to 100. An RSI\n",
    "    above 70 is generally considered overbought, while an RSI below 30 is considered oversold.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframes (dict): A dictionary of DataFrames keyed by ticker symbols\n",
    "    - column (str, optional): The column name to calculate RSI for. Defaults to 'Adjusted_Close'.\n",
    "    - period (int, optional): The period over which to calculate RSI, typically 14. Defaults to 14.\n",
    "\n",
    "    Returns:\n",
    "    - dict: The same dictionary of DataFrames, now including an 'RSI' column with the calculated RSI\n",
    "      values for each stock.\n",
    "\n",
    "    Note:\n",
    "    This function iterates over each DataFrame, calculating the RSI based on price changes in the specified\n",
    "    column. It handles positive and negative changes separately to compute average gains and losses, which\n",
    "    are then used to derive the RSI values.\n",
    "    \"\"\"\n",
    "    \n",
    "    for ticker in dataframes:\n",
    "        df = dataframes[ticker]\n",
    "\n",
    "        # Calculate price difference from the previous day\n",
    "        delta = df[column].diff(1)\n",
    "        # Isolate gains and losses from the delta\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "\n",
    "        # Calculate the Relative Strength (RS)\n",
    "        rs = gain / loss\n",
    "        df['RSI'] = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        dataframes[ticker] = df\n",
    "\n",
    "    return dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "475fad1f-0dc1-4dc6-b550-74c6b8e4b43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = Calculate_RSI(dataframes, column='Adjusted_Close', period=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40964562-7376-4ffc-8435-e4baa4a703f4",
   "metadata": {},
   "source": [
    "## Phase 6: Trend Analysis - Implementing MACD\n",
    "\n",
    "In this phase, the dataset is enhanced with the inclusion of the MACD and its signal line, tools that are utilized to observe and predict potential shifts in stock price movements. The MACD is derived by examining the difference between two averages of stock prices over differing time spans, one short-term and the other long-term, while the signal line, an additional average, serves to validate the insights provided by the MACD.\n",
    "\n",
    "The calculate_macd function is employed to perform these calculations for each stock within the dataset, resulting in the addition of two columns: 'MACD' and 'Signal_Line'. These enhancements facilitate a simpler understanding of when a stock’s price trend may be about to change, offering a basis for more informed buying or selling decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "115dd5f4-2c41-4b03-8543-a70376d081fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_MACD(dataframes, column='Adjusted_Close', short_period=12, long_period=26, signal_period=9):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates the Moving Average Convergence Divergence (MACD) and its signal line for the specified\n",
    "    column in each DataFrame within a collection. MACD is a trend-following momentum indicator\n",
    "    that demonstrates the relationship between two moving averages of a stock's price. The MACD is\n",
    "    calculated by subtracting the long-term Exponential Moving Average (EMA) from the short-term EMA,\n",
    "    with the signal line being an EMA of the MACD itself.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframes (dict): A dictionary of DataFrames keyed by ticker symbols).\n",
    "    - column (str, optional): The column name to calculate MACD for. Defaults to 'Adjusted_Close'.\n",
    "    - short_period (int, optional): The period for the short-term EMA. Defaults to 12.\n",
    "    - long_period (int, optional): The period for the long-term EMA. Defaults to 26.\n",
    "    - signal_period (int, optional): The period for the signal line EMA. Defaults to 9.\n",
    "\n",
    "    Returns:\n",
    "    - dict: The same dictionary of DataFrames, now including 'MACD' and 'Signal_Line' columns with\n",
    "      the calculated values.\n",
    "\n",
    "    Note:\n",
    "    This function adds two columns to each DataFrame: 'MACD', which is the difference between the short-term\n",
    "    and long-term EMAs of the specified column, and 'Signal_Line', which is the EMA of the 'MACD' column over\n",
    "    the specified signal period. These indicators are commonly used to identify potential trend changes and\n",
    "    trading opportunities.\n",
    "    \"\"\"\n",
    "\n",
    "    for ticker in dataframes:\n",
    "        df = dataframes[ticker]\n",
    "        \n",
    "        # Calculate the short-term and long-term EMAs\n",
    "        short_ema = df[column].ewm(span=short_period, adjust=False).mean()\n",
    "        long_ema = df[column].ewm(span=long_period, adjust=False).mean()\n",
    "        \n",
    "        # Compute the MACD by subtracting the long-term EMA from the short-term EMA\n",
    "        df['MACD'] = short_ema - long_ema\n",
    "        \n",
    "        # Calculate the signal line, which is the EMA of the MACD\n",
    "        df['Signal_Line'] = df['MACD'].ewm(span=signal_period, adjust=False).mean()\n",
    "\n",
    "        dataframes[ticker] = df\n",
    "\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e92d8861-47ea-46d8-8aa7-23dd3c8576ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = Calculate_MACD(dataframes, column='Adjusted_Close', short_period=12, long_period=26, signal_period=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063c0ed5-3b19-42e5-ab53-92edee9ce722",
   "metadata": {},
   "source": [
    "## Phase 7: Enhancing Analysis with Bollinger Bands\n",
    "\n",
    "In this phase, the dataset is further augmented through the calculation of Bollinger Bands for each stock, employing a standard method that uses a specified moving average period (often 20 days) and a set number of standard deviations (typically 2) from this average to define upper and lower bands around the stock's price movements. The inclusion of these bands in the dataset, represented by 'Upper_Band' and 'Lower_Band' columns, serves as a mechanism to gauge market volatility and the relative positioning of stock prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30633a29-0619-435e-be07-16f9047b7f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_Bollinger_Bands(dataframes, column='Adjusted_Close', period=20):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates Bollinger Bands for the specified column in each DataFrame within a collection.\n",
    "    Bollinger Bands are a volatility indicator and consist of three lines: the middle band is an N-period\n",
    "    moving average (MA), the upper band is K standard deviations above the MA, and the lower band is K\n",
    "    standard deviations below the MA. This function sets N to the specified period (commonly 20) and K\n",
    "    to 2, following the standard practice.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframes (dict): A dictionary of DataFrames keyed by ticker symbols.\n",
    "    - column (str, optional): The column name to calculate Bollinger Bands for. Defaults to 'Adjusted_Close'.\n",
    "    - period (int, optional): The period over which the moving average and standard deviation are calculated.\n",
    "      Defaults to 20.\n",
    "\n",
    "    Returns:\n",
    "    - dict: The same dictionary of DataFrames, now including 'Upper_Band' and 'Lower_Band' columns with\n",
    "      the calculated Bollinger Bands.\n",
    "\n",
    "    Note:\n",
    "    This function enhances each DataFrame with two new columns: 'Upper_Band' and 'Lower_Band', representing\n",
    "    the calculated upper and lower Bollinger Bands, respectively. These bands are used to assess whether\n",
    "    prices are high or low on a relative basis and can indicate potential market volatility or price\n",
    "    consolidations.\n",
    "    \"\"\"\n",
    "\n",
    "    for ticker in dataframes:\n",
    "        df = dataframes[ticker]\n",
    "        \n",
    "        # Calculate the moving average (MA) and standard deviation (STD) for the specified period\n",
    "        MA = df[column].rolling(window=period).mean()\n",
    "        STD = df[column].rolling(window=period).std()\n",
    "\n",
    "        # Compute the upper and lower Bollinger Bands\n",
    "        df['Upper_Band'] = MA + (STD * 2)\n",
    "        df['Lower_Band'] = MA - (STD * 2)\n",
    "\n",
    "        dataframes[ticker] = df\n",
    "\n",
    "    return dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb912f72-d7a5-4648-ab86-61e0344a1542",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = Calculate_Bollinger_Bands(dataframes, column='Adjusted_Close', period=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e0eb17-48b8-4d8b-b745-ebb38c2e5e21",
   "metadata": {},
   "source": [
    "## Phase 8: Expanding the Dataset with Feature Engineering\n",
    "\n",
    "In this phase, the dataset undergoes an enhancement through the feature engineering. Engineer_Features function enriches each DataFrame with new variables that capture both the momentum and the temporal dynamics of stock prices.\n",
    "\n",
    "This augmentation includes the computation of short-term and medium-term moving averages to pinpoint trends, the calculation of On-Balance Volume (OBV) to gauge buying and selling pressure, and the projection of future price targets across multiple time horizons to assist in predictive modeling. Moreover, the incorporation of time-based features such as the day of the week and month embeds temporal patterns into the analysis, offering insights into periodic fluctuations in stock activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d24de61-b756-48a8-a867-ab37ecdc70c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Engineer_Features(dataframes):\n",
    "\n",
    "    \"\"\"\n",
    "    Enriches each DataFrame with additional features useful for financial analysis and machine learning models.\n",
    "    Features include moving averages, On-Balance Volume (OBV), future price targets for various horizons,\n",
    "    and time-based features like day of the week and month.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframes (dict): A dictionary of DataFrames keyed by ticker symbols.\n",
    "\n",
    "    Returns:\n",
    "    - dict: The same dictionary of pandas DataFrames, now augmented with additional features including\n",
    "      moving averages (MA10, MA20), OBV, future targets (Target_1, Target_2, Target_7, Target_14, Target_28),\n",
    "      and time-based features (Day_of_Week, Month).\n",
    "\n",
    "    Note:\n",
    "    The function aims to facilitate the identification of trends, prediction of future prices, and\n",
    "    capturing of temporal patterns in stock data.\n",
    "    \"\"\"\n",
    "    \n",
    "    for ticker in dataframes:\n",
    "        df = dataframes[ticker]\n",
    "\n",
    "        # Calculate 10-day and 20-day moving averages\n",
    "        df['MA10'] = df['Adjusted_Close'].rolling(window=10).mean()\n",
    "        df['MA20'] = df['Adjusted_Close'].rolling(window=20).mean()\n",
    "        \n",
    "        # Compute On-Balance Volume (OBV)\n",
    "        df['OBV'] = (np.sign(df['Adjusted_Close'].diff()) * df['Volume']).fillna(0).cumsum()\n",
    "\n",
    "        # Define target variables for various prediction horizons\n",
    "        df['Target_1'] = df['Adjusted_Close'].shift(-1)\n",
    "        df['Target_2'] = df['Adjusted_Close'].shift(-2)\n",
    "        df['Target_7'] = df['Adjusted_Close'].shift(-7)\n",
    "        df['Target_14'] = df['Adjusted_Close'].shift(-14)\n",
    "        df['Target_28'] = df['Adjusted_Close'].shift(-28)\n",
    "\n",
    "        # Extract day of the week and month from the 'Date' column for time-based features\n",
    "        df['Day_of_Week'] = df['Date'].dt.dayofweek\n",
    "        df['Month'] = df['Date'].dt.month\n",
    "        \n",
    "        dataframes[ticker] = df\n",
    "\n",
    "    return dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f334a509-e8da-47ce-8f77-bf7be30fef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = Engineer_Features(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fa640a-9217-4b19-8b07-f80825be42f4",
   "metadata": {},
   "source": [
    "## Phase 9: Consolidating Market Indices Data\n",
    "\n",
    "In this phase, the focus shifts towards assembling a comprehensive view of the market by fetching, cleaning, and processing historical data for a predefined set of major stock indices. The Use_Market_Data function stands at the core of this process, tasked with retrieving data over a specific date range, ensuring its quality by filtering based on the allowed percentage of NaN values in the 'Close' column, and calculating adjusted closing prices that account for corporate actions like stock splits and dividends.\n",
    "\n",
    "The function operates on a dictionary of stock indices, fetching data for each through the yfinance library, and then rigorously checks for data completeness and the extent of missing values. Indices failing to meet the set criteria are excluded to maintain the integrity of the analysis. For those passing the checks, the function proceeds to adjust closing prices, merge the data into a unified DataFrame, and fill any gaps, thus crafting a dataset that reflects a holistic and accurate picture of market behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19ef3ad1-04c2-4015-9073-af7f2af7f784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Use_Market_Data(dataframes, start_date, end_date, NaN_threshold_for_close=10):\n",
    "\n",
    "    \"\"\"\n",
    "    Retrieves, cleans, and processes historical market data for a set of predefined stock indices\n",
    "    over a specified date range. The function filters the data based on a threshold for the percentage\n",
    "    of NaN values allowed in the 'Close' column and the expected data completeness. It calculates adjusted\n",
    "    closing prices for each index, merges the data into a single DataFrame, and fills any missing values.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframes (dict): A dictionary to store the adjusted price DataFrames, keyed by index symbols.\n",
    "    - start_date (str): The start date for the data retrieval in \"YYYY-MM-DD\" format.\n",
    "    - end_date (str): The end date for the data retrieval in \"YYYY-MM-DD\" format.\n",
    "    - NaN_threshold_for_close (int, optional): The maximum percentage of NaN values allowed in the 'Close'\n",
    "      column before an index is excluded. Defaults to 10%.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing the adjusted closing prices for each index, with dates aligned\n",
    "      and missing values interpolated.\n",
    "\n",
    "    Note:\n",
    "    The function leverages the yfinance library to fetch the data, filters based on the specified criteria,\n",
    "    and uses custom logic to adjust prices for stock splits and dividends.\n",
    "    \"\"\"\n",
    "\n",
    "    # Predefined stock indices symbols\n",
    "    stock_indices_dict = {'Dow Jones Industrial Average': 'DJIA',\n",
    "                          'Nasdaq Composite': 'COMP',\n",
    "                          'Russell 2000': 'RUT',\n",
    "                          'MSCI World Index': 'MSCI',\n",
    "                          'FTSE 100 (Financial Times Stock Exchange 100 Index)':'UKX',\n",
    "                          'Nikkei 225': '^N225',\n",
    "                          'Shanghai Composite Index': '000001.SS',\n",
    "                          'DAX': 'DAX',\n",
    "                          'SPY': 'SPY',\n",
    "                          'VIX': 'VIX'}\n",
    "\n",
    "    # Dictionary to hold the cleaned and processed index data\n",
    "    df_indices = {}\n",
    "    \n",
    "    # Iterate over each index symbol to fetch and clean data\n",
    "    for symbol in stock_indices_dict:\n",
    "        index = stock_indices_dict[symbol]\n",
    "        ind = yf.Ticker(index)\n",
    "        # Fetch historical market data using yfinance\n",
    "        current_data = ind.history(start=start_date, end=end_date)\n",
    "\n",
    "        # Calculate the percentage of NaN values in 'Close' column\n",
    "        closing_NaN = current_data['Close'].isna().mean() * 100\n",
    "        # Calculate the actual number of rows fetched\n",
    "        number_rows = current_data.shape[0]\n",
    "        # Calculate the expected number of rows based on date range and trading days \n",
    "        expected_days = ((250/365)*(datetime.strptime(end_date, \"%Y-%m-%d\") - datetime.strptime(start_date, \"%Y-%m-%d\")).days)*0.8\n",
    "        \n",
    "        # Check if data meets completeness criteria and NaN threshold\n",
    "        if (number_rows>= expected_days) and (closing_NaN < NaN_threshold_for_close):\n",
    "            \n",
    "            # Select relevant columns and fill NaN values\n",
    "            current_data = current_data[['Dividends', 'Stock Splits', 'Close']]\n",
    "            current_data['Dividends'].fillna(0, inplace=True)\n",
    "            current_data['Stock Splits'].fillna(0, inplace=True)\n",
    "            current_data['Close'] = current_data['Close'].interpolate(method='linear').fillna(method='ffill').fillna(method='bfill')\n",
    "            df_indices[index] = current_data\n",
    "        else:\n",
    "            print('No data for ', index, '!')\n",
    "\n",
    "    # DataFrame to aggregate adjusted market data\n",
    "    Adjusted_Market = pd.DataFrame()\n",
    "    if len(df_indices) !=0:\n",
    "        # Adjust prices for stock splits and dividends\n",
    "        df_indices = Calculate_Adjusted_Price(df_indices)\n",
    "\n",
    "        # Merge adjusted prices from each index into a single DataFrame\n",
    "        for ticker in df_indices:\n",
    "            df = df_indices[ticker]\n",
    "            df.rename(columns = {'Adjusted_Close': str('Adjusted_Close'+'_'+ticker)}, inplace=True)\n",
    "            \n",
    "            if Adjusted_Market.empty:\n",
    "                Adjusted_Market = df[['Date', str('Adjusted_Close'+'_'+ticker)]]\n",
    "            else:\n",
    "                Adjusted_Market = pd.merge(Adjusted_Market, df[['Date', str('Adjusted_Close'+'_'+ticker)]], on='Date', how='outer', suffixes=('', ''))\n",
    "\n",
    "    # Interpolate missing values to ensure completeness\n",
    "    Adjusted_Market = Adjusted_Market.interpolate(method='linear').fillna(method='ffill').fillna(method='bfill')   \n",
    "    \n",
    "    return Adjusted_Market\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81e9d6fa-2944-40ee-8ea6-cea9fdb14f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for  DJIA !\n",
      "No data for  COMP !\n",
      "No data for  RUT !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enabi\\AppData\\Local\\Temp\\ipykernel_24808\\2151122429.py:61: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  current_data['Close'] = current_data['Close'].interpolate(method='linear').fillna(method='ffill').fillna(method='bfill')\n",
      "C:\\Users\\enabi\\AppData\\Local\\Temp\\ipykernel_24808\\2151122429.py:61: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  current_data['Close'] = current_data['Close'].interpolate(method='linear').fillna(method='ffill').fillna(method='bfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for  UKX !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enabi\\AppData\\Local\\Temp\\ipykernel_24808\\2151122429.py:61: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  current_data['Close'] = current_data['Close'].interpolate(method='linear').fillna(method='ffill').fillna(method='bfill')\n",
      "C:\\Users\\enabi\\AppData\\Local\\Temp\\ipykernel_24808\\2151122429.py:61: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  current_data['Close'] = current_data['Close'].interpolate(method='linear').fillna(method='ffill').fillna(method='bfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for  VIX !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enabi\\AppData\\Local\\Temp\\ipykernel_24808\\2151122429.py:61: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  current_data['Close'] = current_data['Close'].interpolate(method='linear').fillna(method='ffill').fillna(method='bfill')\n",
      "C:\\Users\\enabi\\AppData\\Local\\Temp\\ipykernel_24808\\2151122429.py:83: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  Adjusted_Market = Adjusted_Market.interpolate(method='linear').fillna(method='ffill').fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "Adjusted_Market = Use_Market_Data(dataframes, start_date, end_date, NaN_threshold_for_close=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a2a43d-083f-4c7d-a246-26664b8fb6c4",
   "metadata": {},
   "source": [
    "## Phase 10: Integrating Stock and Market Data\n",
    "\n",
    "This phase involves the combination of individual stock data with aggregated market index data, a process executed by the Merge_stock_with_Market function. By aligning both datasets by date, each stock's performance is contextualized within the broader market environment, enhancing the depth and relevance of analyses. This merging not only allows for a direct comparison between a stock's movements and overall market trends but also enriches the stock dataset with a layer of market insight critical for comprehensive analysis.\n",
    "\n",
    "Employing a 'left' join ensures the preservation of the original stock data structure while integrating adjusted market index information, thus maintaining the integrity of individual stock analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "302f0ec1-a147-41bd-89db-07c20dee7fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Merge_stock_with_Market(dataframes, Adjusted_Market):\n",
    "\n",
    "    \"\"\"\n",
    "    Merges adjusted market index data with each individual stock DataFrame in a collection,\n",
    "    aligning them by date. This operation enriches stock data with broader market context,\n",
    "    facilitating analyses that require comparing stock performance against market indices.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframes (dict): A dictionary of DataFrames keyed by ticker symbols.\n",
    "    - Adjusted_Market (pd.DataFrame): A DataFrame containing adjusted market index data, with one column\n",
    "      for dates and other columns for the adjusted closing prices of various market indices.\n",
    "\n",
    "    Returns:\n",
    "    - dict: The original dictionary of DataFrames, now with each stock DataFrame merged with the\n",
    "      Adjusted_Market DataFrame on the 'Date' column.\n",
    "\n",
    "    Note:\n",
    "    The function ensures that the merge operation retains all rows from the stock DataFrames (using a\n",
    "    'left' join).\n",
    "    \"\"\"\n",
    "\n",
    "    # Iterate over each stock DataFrame to merge with market data\n",
    "    for ticker in dataframes:\n",
    "        df = dataframes[ticker]\n",
    "        \n",
    "        # Check if the Adjusted_Market DataFrame contains data\n",
    "        if not Adjusted_Market.empty:\n",
    "            # Merge the stock DataFrame with Adjusted_Market on 'Date', appending market data\n",
    "            df = pd.merge(df, Adjusted_Market, on='Date', how='left', suffixes=('', ''))\n",
    "            # Update the dictionary with the merged DataFrame\n",
    "            dataframes[ticker] = df\n",
    "        \n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c662997-277e-4880-b7fa-4b5c60a03212",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = Merge_stock_with_Market(dataframes, Adjusted_Market)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9dfa9e71-3b06-4eb5-a2a4-6406e839022e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "      <th>Adjusted_Close</th>\n",
       "      <th>RSI</th>\n",
       "      <th>...</th>\n",
       "      <th>Target_7</th>\n",
       "      <th>Target_14</th>\n",
       "      <th>Target_28</th>\n",
       "      <th>Day_of_Week</th>\n",
       "      <th>Month</th>\n",
       "      <th>Adjusted_Close_MSCI</th>\n",
       "      <th>Adjusted_Close_^N225</th>\n",
       "      <th>Adjusted_Close_000001.SS</th>\n",
       "      <th>Adjusted_Close_DAX</th>\n",
       "      <th>Adjusted_Close_SPY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-11-11</td>\n",
       "      <td>31.440089</td>\n",
       "      <td>31.515165</td>\n",
       "      <td>31.156469</td>\n",
       "      <td>31.356672</td>\n",
       "      <td>26872500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.788608</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>25.865033</td>\n",
       "      <td>26.820667</td>\n",
       "      <td>25.669716</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>33.815454</td>\n",
       "      <td>14269.839844</td>\n",
       "      <td>2109.470947</td>\n",
       "      <td>16.194633</td>\n",
       "      <td>119.930829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-11-12</td>\n",
       "      <td>31.181494</td>\n",
       "      <td>31.365011</td>\n",
       "      <td>31.031342</td>\n",
       "      <td>31.164810</td>\n",
       "      <td>31651600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.630815</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>26.088255</td>\n",
       "      <td>26.723015</td>\n",
       "      <td>25.544160</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>33.920666</td>\n",
       "      <td>14588.679688</td>\n",
       "      <td>2126.771973</td>\n",
       "      <td>16.194633</td>\n",
       "      <td>119.687370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-11-13</td>\n",
       "      <td>30.847814</td>\n",
       "      <td>31.832142</td>\n",
       "      <td>30.781082</td>\n",
       "      <td>31.832142</td>\n",
       "      <td>44957600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.179647</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>26.206834</td>\n",
       "      <td>27.162465</td>\n",
       "      <td>25.865033</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>34.301074</td>\n",
       "      <td>14567.160156</td>\n",
       "      <td>2087.940918</td>\n",
       "      <td>16.194633</td>\n",
       "      <td>120.647763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-11-14</td>\n",
       "      <td>31.590239</td>\n",
       "      <td>31.807126</td>\n",
       "      <td>31.465114</td>\n",
       "      <td>31.715366</td>\n",
       "      <td>46183700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.083608</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>26.255660</td>\n",
       "      <td>26.506778</td>\n",
       "      <td>26.116145</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>34.819073</td>\n",
       "      <td>14876.410156</td>\n",
       "      <td>2100.506104</td>\n",
       "      <td>16.194633</td>\n",
       "      <td>121.249714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-11-15</td>\n",
       "      <td>31.656982</td>\n",
       "      <td>31.715374</td>\n",
       "      <td>31.465122</td>\n",
       "      <td>31.565222</td>\n",
       "      <td>50601300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.960125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>26.053365</td>\n",
       "      <td>26.757903</td>\n",
       "      <td>26.011515</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>35.191377</td>\n",
       "      <td>15165.919922</td>\n",
       "      <td>2135.826904</td>\n",
       "      <td>16.194633</td>\n",
       "      <td>121.777288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2512</th>\n",
       "      <td>2023-11-03</td>\n",
       "      <td>348.277763</td>\n",
       "      <td>353.019363</td>\n",
       "      <td>345.986641</td>\n",
       "      <td>351.435486</td>\n",
       "      <td>23624000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>351.435486</td>\n",
       "      <td>66.777660</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>493.610881</td>\n",
       "      <td>32329.185547</td>\n",
       "      <td>3030.798096</td>\n",
       "      <td>27.280834</td>\n",
       "      <td>431.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513</th>\n",
       "      <td>2023-11-06</td>\n",
       "      <td>352.083018</td>\n",
       "      <td>356.157196</td>\n",
       "      <td>351.983399</td>\n",
       "      <td>355.151093</td>\n",
       "      <td>23828300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>355.151093</td>\n",
       "      <td>69.350009</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>492.420882</td>\n",
       "      <td>32708.480469</td>\n",
       "      <td>3058.410889</td>\n",
       "      <td>27.131378</td>\n",
       "      <td>432.586609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2514</th>\n",
       "      <td>2023-11-07</td>\n",
       "      <td>358.009986</td>\n",
       "      <td>361.058148</td>\n",
       "      <td>356.246842</td>\n",
       "      <td>359.135620</td>\n",
       "      <td>25833900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>359.135620</td>\n",
       "      <td>73.299669</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>493.154729</td>\n",
       "      <td>32271.820312</td>\n",
       "      <td>3057.270020</td>\n",
       "      <td>27.041704</td>\n",
       "      <td>433.817749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2515</th>\n",
       "      <td>2023-11-08</td>\n",
       "      <td>360.281147</td>\n",
       "      <td>362.462680</td>\n",
       "      <td>359.155513</td>\n",
       "      <td>361.795288</td>\n",
       "      <td>26767800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>361.795288</td>\n",
       "      <td>73.883761</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>503.569458</td>\n",
       "      <td>32166.480469</td>\n",
       "      <td>3052.373047</td>\n",
       "      <td>27.211088</td>\n",
       "      <td>434.135498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2516</th>\n",
       "      <td>2023-11-09</td>\n",
       "      <td>360.898741</td>\n",
       "      <td>363.379132</td>\n",
       "      <td>358.966242</td>\n",
       "      <td>359.294983</td>\n",
       "      <td>24847300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>359.294983</td>\n",
       "      <td>76.331259</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>504.805969</td>\n",
       "      <td>32646.460938</td>\n",
       "      <td>3053.279053</td>\n",
       "      <td>27.181198</td>\n",
       "      <td>430.749786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2517 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Open        High         Low       Close    Volume  \\\n",
       "0    2013-11-11   31.440089   31.515165   31.156469   31.356672  26872500   \n",
       "1    2013-11-12   31.181494   31.365011   31.031342   31.164810  31651600   \n",
       "2    2013-11-13   30.847814   31.832142   30.781082   31.832142  44957600   \n",
       "3    2013-11-14   31.590239   31.807126   31.465114   31.715366  46183700   \n",
       "4    2013-11-15   31.656982   31.715374   31.465122   31.565222  50601300   \n",
       "...         ...         ...         ...         ...         ...       ...   \n",
       "2512 2023-11-03  348.277763  353.019363  345.986641  351.435486  23624000   \n",
       "2513 2023-11-06  352.083018  356.157196  351.983399  355.151093  23828300   \n",
       "2514 2023-11-07  358.009986  361.058148  356.246842  359.135620  25833900   \n",
       "2515 2023-11-08  360.281147  362.462680  359.155513  361.795288  26767800   \n",
       "2516 2023-11-09  360.898741  363.379132  358.966242  359.294983  24847300   \n",
       "\n",
       "      Dividends  Stock Splits  Adjusted_Close        RSI  ...   Target_7  \\\n",
       "0           0.0           0.0       25.788608        NaN  ...  25.865033   \n",
       "1           0.0           0.0       25.630815        NaN  ...  26.088255   \n",
       "2           0.0           0.0       26.179647        NaN  ...  26.206834   \n",
       "3           0.0           0.0       26.083608        NaN  ...  26.255660   \n",
       "4           0.0           0.0       25.960125        NaN  ...  26.053365   \n",
       "...         ...           ...             ...        ...  ...        ...   \n",
       "2512        0.0           0.0      351.435486  66.777660  ...        NaN   \n",
       "2513        0.0           0.0      355.151093  69.350009  ...        NaN   \n",
       "2514        0.0           0.0      359.135620  73.299669  ...        NaN   \n",
       "2515        0.0           0.0      361.795288  73.883761  ...        NaN   \n",
       "2516        0.0           0.0      359.294983  76.331259  ...        NaN   \n",
       "\n",
       "      Target_14  Target_28  Day_of_Week  Month  Adjusted_Close_MSCI  \\\n",
       "0     26.820667  25.669716            0     11            33.815454   \n",
       "1     26.723015  25.544160            1     11            33.920666   \n",
       "2     27.162465  25.865033            2     11            34.301074   \n",
       "3     26.506778  26.116145            3     11            34.819073   \n",
       "4     26.757903  26.011515            4     11            35.191377   \n",
       "...         ...        ...          ...    ...                  ...   \n",
       "2512        NaN        NaN            4     11           493.610881   \n",
       "2513        NaN        NaN            0     11           492.420882   \n",
       "2514        NaN        NaN            1     11           493.154729   \n",
       "2515        NaN        NaN            2     11           503.569458   \n",
       "2516        NaN        NaN            3     11           504.805969   \n",
       "\n",
       "      Adjusted_Close_^N225  Adjusted_Close_000001.SS  Adjusted_Close_DAX  \\\n",
       "0             14269.839844               2109.470947           16.194633   \n",
       "1             14588.679688               2126.771973           16.194633   \n",
       "2             14567.160156               2087.940918           16.194633   \n",
       "3             14876.410156               2100.506104           16.194633   \n",
       "4             15165.919922               2135.826904           16.194633   \n",
       "...                    ...                       ...                 ...   \n",
       "2512          32329.185547               3030.798096           27.280834   \n",
       "2513          32708.480469               3058.410889           27.131378   \n",
       "2514          32271.820312               3057.270020           27.041704   \n",
       "2515          32166.480469               3052.373047           27.211088   \n",
       "2516          32646.460938               3053.279053           27.181198   \n",
       "\n",
       "      Adjusted_Close_SPY  \n",
       "0             119.930829  \n",
       "1             119.687370  \n",
       "2             120.647763  \n",
       "3             121.249714  \n",
       "4             121.777288  \n",
       "...                  ...  \n",
       "2512          431.593750  \n",
       "2513          432.586609  \n",
       "2514          433.817749  \n",
       "2515          434.135498  \n",
       "2516          430.749786  \n",
       "\n",
       "[2517 rows x 29 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes['MSFT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f14e8918-10b3-4bd4-8a64-52c550cccc87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends',\n",
       "       'Stock Splits', 'Adjusted_Close', 'RSI', 'MACD', 'Signal_Line',\n",
       "       'Upper_Band', 'Lower_Band', 'MA10', 'MA20', 'OBV', 'Target_1',\n",
       "       'Target_2', 'Target_7', 'Target_14', 'Target_28', 'Day_of_Week',\n",
       "       'Month', 'Adjusted_Close_MSCI', 'Adjusted_Close_^N225',\n",
       "       'Adjusted_Close_000001.SS', 'Adjusted_Close_DAX', 'Adjusted_Close_SPY'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes['MSFT'].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6f9397-a861-437e-a897-ace132efb272",
   "metadata": {},
   "source": [
    "## Phase 11: Exporting DataFrames to CSV\n",
    "\n",
    "In this step, the refined stock data is saved into CSV files, with one file corresponding to each stock. Through the Build_DataFrames function, a CSV file is created for each entry in the stock data dictionary. The file names are composed of the stock's symbol and the specified date range, ensuring clarity and ease of identification.\r\n",
    "\r\n",
    "Here’s what happens: A file name is generated for each stock, incorporating the stock's symbol and the date range of interest. A check is performed to see if a 'Dfs' folder, intended for storing these files, exists within the notebook's current location. If not found, this folder is automatically created. Subsequently, each stock's data is saved into a uniquely named CSV file within this foldeork."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21e9f66c-c1a7-4a44-a505-7dc6f6a7b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_DataFrames(dataframes, start_date, end_date):\n",
    "    \n",
    "    \"\"\"\n",
    "    Saves each DataFrame in the provided dictionary to a CSV file named after the ticker symbol.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframes (dict): A dictionary where each key is a ticker symbol (string) and each value is a pandas DataFrame.\n",
    "      The function iterates through this dictionary, saving each DataFrame to a CSV file.\n",
    "\n",
    "    Note:\n",
    "    This function does not return a value. Instead, it writes files to the disk, saving each DataFrame in the 'dataframes'\n",
    "    dictionary to a separate CSV file.\n",
    "    \"\"\"\n",
    "    \n",
    "    for ticker in dataframes:\n",
    "        # Concatenate the ticker symbol with '.csv' to form the filename        \n",
    "        name_df = str(ticker+'_'+start_date+'_'+end_date+'.csv')\n",
    "        \n",
    "        # Get the current working directory (where your notebook is located)\n",
    "        current_directory = os.getcwd()\n",
    "        \n",
    "        # Name of the subfolder where you want to save the CSV file\n",
    "        subfolder_name = 'Dfs'\n",
    "\n",
    "        # Construct the full path to the subfolder\n",
    "        subfolder_path = os.path.join(current_directory, subfolder_name)\n",
    "        \n",
    "        # Check if the subfolder exists, if not, create it\n",
    "        if not os.path.exists(subfolder_path):\n",
    "            os.makedirs(subfolder_path)\n",
    "\n",
    "        # Full path for the CSV file\n",
    "        file_path = os.path.join(subfolder_path, name_df)\n",
    "\n",
    "        # Save the DataFrame associated with the current ticker to a CSV file, excluding the index\n",
    "        dataframes[ticker].to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b9946b9-f558-4a1e-9a78-74d4d34891fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Build_DataFrames(dataframes, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67be8a32-bd8b-4d34-aa80-79aea15aa1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
