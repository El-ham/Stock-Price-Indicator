{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96164879-2312-4e92-b1fe-4baa18c65b43",
   "metadata": {},
   "source": [
    "## Essential Libraries for Data Science and Machine Learning\n",
    "\n",
    "Essential Python libraries is imported, each tailored for specific roles within data science and machine learning workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f664bedc-7391-48de-abdd-97556e9ef149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pickle\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67f447eb-b2e5-4603-8834-3cedfd1bac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_ticker_symbols = ['MSFT', 'UPS']\n",
    "start_date = '2023-07-10'\n",
    "end_date = '2023-11-10'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9a605a-c141-4c87-adee-3a2b63c8c1a8",
   "metadata": {},
   "source": [
    "## Step 1: Loading Financial Data\n",
    "\n",
    "The Read_Data function serves as a step for loading financial data into a Python analysis environment. It automates the retrieval of historical data for a specified financial instrument, identified by its ticker, over a given date range from a CSV file. By constructing a file path based on the ticker symbol, start date, and end date, it locates the correct CSV file within a predetermined directory (Dfs). The data is then loaded into a pandas DataFrame, providing a structured format suitable for subsequent data analysis and manipulation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "335dd939-bfa0-4976-a403-e5272dc8523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_Data(ticker, start_date, end_date):\n",
    "\n",
    "    \"\"\"\n",
    "    Reads data from a CSV file into a pandas DataFrame.\n",
    "    \n",
    "    This function constructs a file path from a specified ticker symbol, start date, and end date,\n",
    "    then reads the corresponding CSV file from the 'Dfs' folder within the current working directory.\n",
    "    \n",
    "    Parameters:\n",
    "    - ticker (str): The ticker symbol for the data file.\n",
    "    - start_date (str): The start date of the data range.\n",
    "    - end_date (str): The end date of the data range.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing the data from the CSV file.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Name of the folder where your CSV file is located\n",
    "    folder_name = 'Dfs'\n",
    "    \n",
    "    # Construct the file name using the provided parameters.\n",
    "    file_name = ticker+'_'+start_date+'_'+end_date+'.csv'\n",
    "    \n",
    "    # Retrieve the current working directory to build the full file path.\n",
    "    current_directory = os.getcwd()\n",
    "    \n",
    "    # Construct the full path to the CSV file\n",
    "    file_path = os.path.join(current_directory, folder_name, file_name)\n",
    "    \n",
    "    # Read and return the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926d41c1-4610-42bf-91ee-b400e68a9f3b",
   "metadata": {},
   "source": [
    "## Step 2: Preparing Data for Analysis\n",
    "\n",
    "The Finalize_df function is a step for preparing the loaded financial data for further analysis. It focuses on cleaning the data by performing two main tasks: removing unnecessary columns related to target predictions that do not match the specified analysis interval, and eliminating any rows that contain missing values (NaN). This process ensures the integrity and relevance of the data. Additionally, it identifies the column that will be used for target predictions, aligning with the chosen interval, making the data set ready for predictive modeling or any subsequent analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e675ebb-3811-4818-a375-754209ae15c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Finalize_df(df, ticker, start_date, end_date, interval):\n",
    "\n",
    "    \"\"\"\n",
    "    Cleans the DataFrame by dropping specific columns and rows with NaN values, and identifies the target column.\n",
    "    \n",
    "    This function iterates through the DataFrame's columns to remove any that contain the word 'Target' \n",
    "    but do not match the specified interval. It then identifies the remaining 'Target' column and \n",
    "    removes any rows in the DataFrame that contain NaN values, ensuring data integrity for further analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame to be cleaned and finalized.\n",
    "    - interval (int): The interval value used to identify the relevant 'Target' column to retain.\n",
    "    \n",
    "    Returns:\n",
    "    - tuple: A tuple containing:\n",
    "        - The name of the target column (str).\n",
    "        - The cleaned DataFrame (pd.DataFrame) with irrelevant target columns removed and no NaN values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Identify columns to drop: those including 'Target' in their name but not matching the specified interval.\n",
    "    columns_to_drop = [col for col in df.columns if 'Target' in col and 'Target_'+str(interval) != col]\n",
    "    \n",
    "    # Drop identified columns from the DataFrame.\n",
    "    dft = df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    # Extract the name of the remaining 'Target' column.\n",
    "    Target = [col for col in dft.columns if 'Target' in col][0]\n",
    "\n",
    "    # Get the last row of the DataFrame\n",
    "    last_row = dft.tail(1)\n",
    "    del last_row[Target]\n",
    "    del last_row['Date']\n",
    "    \n",
    "    try:\n",
    "        # Save the last row to a CSV file for future predictions\n",
    "        last_row.to_csv(f\"Last_Rows/last_row_of_{ticker}_{start_date}_{end_date}.csv\", index=False)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        # Handle the case where the 'Last_Rows' directory does not exist.\n",
    "        print(\"Error: The specified directory 'Last_Rows' does not exist.\")\n",
    "    \n",
    "    # Remove rows with any NaN values across all columns to ensure data completeness.\n",
    "    df_cleaned = dft.dropna()\n",
    "\n",
    "    return Target, df_cleaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefba193-bd56-4e93-99a4-1ea944440a30",
   "metadata": {},
   "source": [
    "## Step 3: Segmenting Data for Model Training and Testing\n",
    "\n",
    "The Split_train_test function is crucial for dividing the cleaned dataset into appropriate subsets for training and testing machine learning models. It first segregates the dataset into features (independent variables) and the target (dependent variable) based on the column names. It excludes non-feature columns like 'Date' and the specified target column from the features set. Utilizing a predefined proportion (e.g., 80% training, 20% testing) and a fixed random state to ensure the splits are reproducible, it then divides these datasets into training and testing sets. This step is necessary for evaluating the performance of predictive models in a controlled and consistent manner, helping to ensure that the model can generalize well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7cd60e91-2127-4559-aea9-2b286ff9f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Split_train_test(df, Target):\n",
    "\n",
    "    \"\"\"\n",
    "    Splits the DataFrame into training and testing datasets for features and target.\n",
    "\n",
    "    The function separates the DataFrame into features (X) and target (y) datasets by dropping \n",
    "    the specified target column and any non-feature columns (e.g., 'Date'). It then splits these \n",
    "    datasets into training and testing sets using a predefined test size and random state to ensure \n",
    "    reproducibility.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame containing the dataset to be split.\n",
    "    - Target (str): The name of the target column in the DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    - tuple: Contains the training and testing sets for both features (X_train, X_test) and \n",
    "      target (y_train, y_test).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare the features dataset by dropping the target column and other non-numeric/non-feature columns.\n",
    "    X = df.drop([Target, 'Date'], axis=1)\n",
    "    # Extract the target column as the target dataset.\n",
    "    y = df[Target]\n",
    "    \n",
    "     # Split the data into training and testing sets, allocating 20% of the data to the testing set and\n",
    "    # setting a random state for reproducibility.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea023d6-3e10-40ae-813d-15e50f48615e",
   "metadata": {},
   "source": [
    "## Step 4: Building and Evaluating\n",
    "### The Linear Regression Model\n",
    "\n",
    "The Model_with_LR function marks a essential step in the project: the construction and evaluation of a Linear Regression model. After preparing the data, this function leverages the training set to train a Linear Regression model, applying it thereafter to the testing set to forecast outcomes. The function assesses the model's accuracy and efficiency by computing three key metrics: the Mean Squared Error (MSE), Mean Absolute Percentage Error (MAPE) and the R-squared (R²) score. These metrics provide insights into the model's predictive power and the variance it explains, respectively, thereby enabling a quantitative evaluation of the model's performance in predicting the target variable based on the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "74790ede-414b-4a27-8551-502f990be6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_with_LR(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    \"\"\"\n",
    "    Initializes, fits, and evaluates a Linear Regression model using the provided training and testing datasets.\n",
    "    \n",
    "\n",
    "    This function creates a Ridge Regression model, leveraging Grid Search with Cross Validation to optimize the \n",
    "    regularization strength (alpha). The model is fitted to the training data, and then predictions are made on the \n",
    "    testing set. The performance is evaluated using the Mean Squared Error (MSE), Mean Absolute Percentage Error (MAPE), \n",
    "    and R-squared (R2) score, assessing how well the actual outcomes are predicted.\n",
    "    \n",
    "    \n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): The training dataset containing the features.\n",
    "    - X_test (pd.DataFrame): The testing dataset containing the features.\n",
    "    - y_train (pd.Series): The training dataset containing the target variable.\n",
    "    - y_test (pd.Series): The testing dataset containing the target variable.\n",
    "    \n",
    "    Returns:\n",
    "    - tuple: Contains the fitted model using GridSearchCV, MSE, MAPE, and the R2 score.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Define the parameter grid for Ridge Regression\n",
    "    param_grid = {\n",
    "        'alpha': [0.1, 1, 10, 100]\n",
    "    }\n",
    "    \n",
    "    # Initialize Ridge Regression model\n",
    "    model = Ridge()\n",
    "    \n",
    "    # Initialize GridSearchCV\n",
    "    LR = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "    LR.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    # Use the fitted model to make predictions on the test dataset.\n",
    "    y_pred = LR.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model's performance by calculating the Mean Squared Error and R-squared score.\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Calculate the absolute percentage difference\n",
    "    absolute_percentage_differences = np.abs((y_test - y_pred) / y_test) * 100\n",
    "    # Calculate the mean of these absolute percentage differences\n",
    "    mape = np.mean(absolute_percentage_differences)\n",
    "    print(mse, r2, mape)\n",
    "    \n",
    "    return LR, mse, r2, mape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a93116-475e-45e1-b43a-a1e589206459",
   "metadata": {},
   "source": [
    "### The XGBoost Regression Model\n",
    "\n",
    "The Model_with_XGB function deploys an XGBoost regression model to the project. After preparing the dataset, this function configures an XGBoost model with pre-defined hyperparameters, trains it on the dataset, and evaluates its predictive performance using the test data. The function focuses on understanding the model's accuracy through Mean Squared Error (MSE), Mean Absolute Percentage Error (MAPE) and R-squared (R²) metrics, crucial for assessing the model's effectiveness in capturing the complexity of the data and making accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af0ea614-d25b-427c-bc6e-5be8bc04c602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_with_XGB(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    \"\"\"\n",
    "    Initializes, fits, and evaluates an XGBoost regression model with optimized hyperparameters through Grid Search and Cross-Validation.\n",
    "    \n",
    "\n",
    "    This function sets up an XGBoost regression model and uses Grid Search with 3-fold Cross-Validation to systematically explore a range \n",
    "    of hyperparameters including the number of estimators, learning rate, max depth, and subsample ratio. It aims to identify\n",
    "    the optimal settings by minimizing the negative mean squared error. The model, once fitted with the best parameters, is then used to \n",
    "    predict on the test dataset. Performance metrics calculated include Mean Squared Error (MSE), Mean Absolute Percentage Error (MAPE), \n",
    "    and R-squared (R2), providing insights into the accuracy and predictive quality of the model.\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): The training dataset containing the features.\n",
    "    - X_test (pd.DataFrame): The testing dataset containing the features.\n",
    "    - y_train (pd.Series): The training dataset containing the target variable.\n",
    "    - y_test (pd.Series): The testing dataset containing the target variable.\n",
    "    \n",
    "    Returns:\n",
    "    - tuple: Contains the fitted XGBoost model, the MSE, MAPE, and the R2 score.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "\n",
    "    # Initialize the XGBoost regressor model\n",
    "    xgb = XGBRegressor(random_state=42)\n",
    "\n",
    "    # Initialize GridSearchCV with the parameter grid and fit it to the training data\n",
    "    xgb_model = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "        \n",
    "    # Use the trained model to make predictions on the test dataset.\n",
    "    xgb_pred = xgb_model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model's performance on the test data using Mean Squared Error and R-squared metrics.\n",
    "    xgb_mse = mean_squared_error(y_test, xgb_pred)\n",
    "    xgb_r2 = r2_score(y_test, xgb_pred)\n",
    "\n",
    "    # Calculate the absolute percentage difference\n",
    "    absolute_percentage_differences = np.abs((y_test - xgb_pred) / y_test) * 100\n",
    "    # Calculate the mean of these absolute percentage differences\n",
    "    mape = np.mean(absolute_percentage_differences)\n",
    "\n",
    "    print(xgb_mse, xgb_r2, mape)\n",
    "    return xgb_model, xgb_mse, xgb_r2, mape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61237219-16a5-4f08-9145-abe92c0f1bbc",
   "metadata": {},
   "source": [
    "### The Random Forest Regression Model\n",
    "\n",
    "The Model_with_RF function incorporates the use of a Random Forest regression model into the project, marking a move towards leveraging ensemble learning techniques for enhanced predictive modeling. By initiating a Random Forest model with its default settings, the function trains the model on the dataset prepared in previous steps and assesses its predictive power on the test set. The evaluation focuses on the Mean Squared Error (MSE), Mean Absolute Percentage Error (MAPE) and R-squared (R²) metrics to gauge the model's accuracy and the proportion of variance it explains in the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f00e94d2-34ea-4e00-ba1d-010f58b31a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_with_RF(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    \"\"\"\n",
    "    Initializes, fits, and evaluates a Random Forest regression model with optimized hyperparameters through Grid Search and Cross-Validation.\n",
    "    \n",
    "    This function sets up a Random Forest regression model and applies Grid Search with 3-fold Cross-Validation\n",
    "    to systematically explore various combinations of hyperparameters such as the number of trees (n_estimators), \n",
    "    tree depth (max_depth), and minimum samples required at a leaf node (min_samples_leaf) and a split (min_samples_split). \n",
    "    This rigorous approach aims to pinpoint the most effective settings to minimize the negative mean squared error. \n",
    "    After identifying the best parameters, the model is fitted to the training data and used for predictions on the testing set. \n",
    "    The model's effectiveness is assessed by calculating the Mean Squared Error (MSE), Mean Absolute Percentage Error (MAPE), \n",
    "    and R-squared (R2) score, offering insights into both the precision and the predictive power of the model.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): The training dataset containing the features.\n",
    "    - X_test (pd.DataFrame): The testing dataset containing the features.\n",
    "    - y_train (pd.Series): The training dataset containing the target variable.\n",
    "    - y_test (pd.Series): The testing dataset containing the target variable.\n",
    "    \n",
    "    Returns:\n",
    "    - tuple: Contains the fitted Random Forest model, the MSE, MAPE, and the R2 score.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    # Initialize the Random Forest regressor model\n",
    "    rf = RandomForestRegressor()\n",
    "\n",
    "    # Initialize GridSearchCV with the parameter grid and fit it to the training data\n",
    "    rf_model = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    \n",
    "    # Use the trained model to make predictions on the test dataset.\n",
    "    y_pred_rf = rf_model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model's performance on the test data using Mean Squared Error and R-squared metrics.\n",
    "    mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "    r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "    # Calculate the absolute percentage difference\n",
    "    absolute_percentage_differences = np.abs((y_test - y_pred_rf) / y_test) * 100\n",
    "    # Calculate the mean of these absolute percentage differences\n",
    "    mape = np.mean(absolute_percentage_differences)\n",
    "\n",
    "    print(mse_rf, r2_rf, mape)\n",
    "    return rf_model, mse_rf, r2_rf, mape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be66a98a-3cce-4cdf-aed3-7d322e22f349",
   "metadata": {},
   "source": [
    "## Step 5: Selecting and Saving the Optimal Model\n",
    "\n",
    "The Save_Best_Model function integrates model comparison and selection, targeting the identification of the most effective regression model for the dataset based on its predictive performance. This function systematically evaluates Linear Regression, XGBoost, and Random Forest models, all previously prepared and assessed, to determine the best model according to mean squared error (MSE), Mean Absolute Percentage Error (MAPE) and R-squared (R²) metrics. Only models surpassing a high threshold of predictive accuracy (MAPE <=5 and R² >= 0.60) are considered for final selection. The best-performing model, if meeting the set criteria, is then serialized (pickled) for persistence, facilitating its reuse without retraining. This step encapsulates the project's goal of optimizing predictive accuracy while providing a mechanism for efficiently deploying the chosen model in future applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "54927333-91f5-4a2d-92d3-f792616ddd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Save_Best_Model(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    \"\"\"\n",
    "    Evaluates multiple regression models on the given dataset, identifies the best performing model based on\n",
    "    mean squared error (MSE) and R-squared (R2) metrics, and pickles the best model if its R2 score is above 0.90.\n",
    "    \n",
    "    The function considers Linear Regression, XGBoost, and Random Forest models, comparing their performance\n",
    "    on the given training and testing datasets. The best model is determined by the lowest MSE, with a tiebreaker\n",
    "    based on the mape<= 5 and R2 score>= 0.60.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): The training dataset containing the features.\n",
    "    - X_test (pd.DataFrame): The testing dataset containing the features.\n",
    "    - y_train (pd.Series): The training dataset containing the target variable.\n",
    "    - y_test (pd.Series): The testing dataset containing the target variable.\n",
    "    \n",
    "    Returns:\n",
    "    - The tuple of the best model, its MSE, MAPE, and R2 score, if its MAPE is below 5% and R2 score is above 0.60; otherwise, None.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize variables to store the best model and its metrics\n",
    "    best_model = None\n",
    "    best_mse = float('inf')  # Initialize with a large value\n",
    "    best_r2 = -float('inf')   # Initialize with a small value\n",
    "\n",
    "    # Generate models using predefined functions.\n",
    "    Model_LR = Model_with_LR(X_train, X_test, y_train, y_test)\n",
    "    Model_XGB = Model_with_XGB(X_train, X_test, y_train, y_test)\n",
    "    Model_RF = Model_with_RF(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # List of models to evaluate\n",
    "    models = {\n",
    "        \"Linear Regression\": Model_LR,\n",
    "        \"XGBoost\": Model_XGB,\n",
    "        \"Random Forest\": Model_RF\n",
    "    }\n",
    "\n",
    "    # Evaluate each model, updating the best model based on MSE and R2.\n",
    "    for model_name, (model, mse, r2, mape) in models.items():\n",
    "\n",
    "        # Check if the current model has better performance than the best model found so far\n",
    "        if mse < best_mse:\n",
    "            best_model = model_name\n",
    "            best_mse = mse\n",
    "            best_r2 = r2\n",
    "            best_mape = mape\n",
    "            \n",
    "    # Conditionally choose the best model if its performance is sufficiently high.\n",
    "    if best_mape<= 5 and best_r2>= 0.60:\n",
    "        \n",
    "        if best_model == \"Linear Regression\":\n",
    "            model_to_pickle = Model_LR[0]\n",
    "        elif best_model == \"XGBoost\":\n",
    "            model_to_pickle = Model_XGB[0]\n",
    "        elif best_model == \"Random Forest\":\n",
    "            model_to_pickle = Model_RF[0]\n",
    "\n",
    "    else:\n",
    "        print('Could not create model with high coverage (Mean Absolute Percentage Error (MAPE) < 5 and R_squared >= 0.60) for the given data. Consider changing input parameters.')\n",
    "        \n",
    "    return model_to_pickle, best_mse, best_r2, best_mape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7c2758-354e-4d61-8ded-d454e974eef5",
   "metadata": {},
   "source": [
    "## Step 6: Persisting the Best Model for Future Use\n",
    "\n",
    "The Make_Pickle function encapsulates the final step in the predictive modeling process: saving the optimal model for future application. By serializing the chosen model into a binary file using Python's pickle module, this function ensures that the model can be easily and efficiently reloaded for subsequent predictions, without the need to retrain. The naming convention for the output file incorporates both the ticker symbol and the interval for which the model was developed, facilitating straightforward identification and retrieval of the model. This step is crucial for operationalizing the model in practical financial analysis scenarios, allowing for quick deployment in predictive tasks related to the specified financial instrument and analysis interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6987879f-de7d-439e-abc4-57baf92e74f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_Pickle(model_to_pickle, interval, ticker, start_date, end_date):\n",
    "\n",
    "    \"\"\"\n",
    "    Pickles and saves the provided model to a file, named according to the specified ticker and interval.\n",
    "    \n",
    "    The function writes the model to a binary file within the 'Pickles' directory. The naming convention for the file\n",
    "    includes the model's associated ticker symbol and interval, ensuring easy identification and retrieval of the model\n",
    "    for future predictions or analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    - model_to_pickle: The model object to be pickled and saved.\n",
    "    - interval (str or int): The interval associated with the model, contributing to the file's name.\n",
    "    - ticker (str): The ticker symbol associated with the model, contributing to the file's name.\n",
    "    \n",
    "    Outputs:\n",
    "    - On success, the model is saved to a .pkl file in the 'Pickles' directory, and a confirmation message is printed.\n",
    "    - On failure, an error message is printed detailing the nature of the problem encountered.\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If the 'Pickles' directory does not exist.\n",
    "    - IOError: If there is an issue writing to the file.\n",
    "    - pickle.PicklingError: If the model object cannot be serialized.\n",
    "    - Exception: Catches any other unexpected errors that occur during execution.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Open the file and pickle the model object to the file.\n",
    "    try:\n",
    "        with open(f\"Pickles/best_model_{ticker}_{interval}_{start_date}_{end_date}.pkl\", \"wb\") as f:\n",
    "            # Serialize and save the 'model_to_pickle' object into the file.\n",
    "            pickle.dump(model_to_pickle, f)\n",
    "        # Print a success message indicating the model has been successfully pickled and saved.    \n",
    "        print(f\"Model pickled successfully for {ticker}_{interval}.\")\n",
    "    except FileNotFoundError:\n",
    "        # Handle the case where the 'Pickles' directory does not exist.\n",
    "        print(\"Error: The specified directory 'Pickles' does not exist.\")\n",
    "    except IOError:\n",
    "        # Handle general input/output errors, such as failure in file writing.\n",
    "        print(\"IOError: There was an issue writing to the file.\")\n",
    "    except pickle.PicklingError:\n",
    "        # Handle errors specific to the pickling process, such as if the object cannot be serialized.\n",
    "        print(\"PicklingError: There was an issue pickling the model.\")\n",
    "    except Exception as e:\n",
    "        # Catch any other unexpected exceptions and print an error message detailing the problem.\n",
    "        print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97387ae5-fa67-47ca-a4ff-79e103064ff0",
   "metadata": {},
   "source": [
    "## Step 7: Automating Model Development and Persistence Across Time Intervals\n",
    "\n",
    "The Make_Predictions function has a comprehensive process tailored for financial data analysis, specifically aimed at generating and saving predictive models for a range of time intervals. This automated pipeline initiates with reading historical stock data for a specified ticker within a given date range. It then iteratively cleans the data, selects the best model based on predefined performance metrics (MSE, MAPE and R-squared score), and finally serializes the optimal model for each interval, making it readily accessible for future predictions.\n",
    "\n",
    "For each interval in the predetermined list, the function executes several key steps: data preparation (cleaning and finalizing), dataset partitioning into training and testing sets, model comparison and selection, and model serialization. This end-to-end process not only enhances efficiency in model development but also ensures that the models are tailored to specific intervals, thereby improving the relevance and accuracy of predictions for different time frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "34112051-96f5-48f0-9148-9b4050005d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_Predictions(ticker, start_date, end_date):\n",
    "\n",
    "    \"\"\"\n",
    "    Processes stock data for a given ticker and date range to train, evaluate, and save the best predictive model\n",
    "    for various time intervals.\n",
    "    \n",
    "    For each specified interval, this function reads historical stock data, cleans it, splits it into training and testing\n",
    "    sets, identifies the best model based on performance metrics, and pickles this model for future use.\n",
    "    \n",
    "    Parameters:\n",
    "    - ticker (str): The stock ticker symbol for which data is processed.\n",
    "    - start_date (str): The start date for the data range to be processed.\n",
    "    - end_date (str): The end date for the data range to be processed.\n",
    "    \n",
    "    Processes:\n",
    "    - For each interval in a predefined list, performs data cleaning, model training and evaluation,\n",
    "      and pickles the best-performing model.\n",
    "    \n",
    "    Outputs:\n",
    "    - Saves pickled models for each interval to the disk.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the list of intervals to generate models for.\n",
    "    List_of_intervals = [1, 2, 7, 14, 28]\n",
    "\n",
    "    # Read the data for the specified ticker and date range.\n",
    "    df = Read_Data(ticker, start_date, end_date)\n",
    "    print(ticker,'\\'s Data frame has been built Successfully!')\n",
    "\n",
    "    # Iterate over each interval, process the data, and save the best model.\n",
    "    for interval in List_of_intervals: \n",
    "        # Prepare the data for the given interval.\n",
    "        Target, df_cleaned = Finalize_df(df, ticker, start_date, end_date, interval)\n",
    "        # Split the data into training and testing sets.\n",
    "        X_train, X_test, y_train, y_test = Split_train_test(df_cleaned, Target)\n",
    "        # Identify and save the best model for the interval.\n",
    "        model_to_pickle, best_mse, best_r2, best_mape = Save_Best_Model(X_train, X_test, y_train, y_test)\n",
    "        # Check if a model was selected to be pickled before attempting to pickle it.\n",
    "        if model_to_pickle:\n",
    "            print(f'Best model selected for {ticker} for predicting {interval} days ahead, with MSE = {best_mse} , Rsquared = {best_r2} and best_mape = {best_mape}')\n",
    "            Make_Pickle(model_to_pickle, interval, ticker, start_date, end_date)\n",
    "        else:\n",
    "            print(f\"No suitable model found for interval {interval} to pickle.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35ed2a9-4d4e-4533-9050-d152027542c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSFT 's Data frame has been built Successfully!\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "11.838580362445963 0.9004826928081129 0.9072753505396282\n",
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
      "11.90596955288934 0.8999162067463179 0.9393489952133082\n",
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n",
      "16.22696191067718 0.8635931417606 0.9721999781282502\n",
      "Best model selected for MSFT for predicting 1 days ahead, with MSE = 11.838580362445963 , Rsquared = 0.9004826928081129 and best_mape = 0.9072753505396282\n",
      "Model pickled successfully for MSFT_1.\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "10.304708852232972 0.9309496727581025 0.8703133283115438\n",
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
      "8.667411787272938 0.9419209578035009 0.7516399249109862\n",
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n",
      "15.339794292695686 0.897210311235091 1.0191254270561392\n",
      "Best model selected for MSFT for predicting 2 days ahead, with MSE = 8.667411787272938 , Rsquared = 0.9419209578035009 and best_mape = 0.7516399249109862\n",
      "Model pickled successfully for MSFT_2.\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "30.963463099415222 0.8332961441869204 1.374581089755299\n",
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enabi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:200: LinAlgWarning: Ill-conditioned matrix (rcond=8.30469e-18): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.707289526403706 0.8723625790099954 1.2266280757459749\n",
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n"
     ]
    }
   ],
   "source": [
    "for t in list_of_ticker_symbols:\n",
    "    Make_Predictions(t, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6b8148-d9ee-4d5f-9fce-a65c688996ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803a7d1b-81a5-465a-b86d-bfeeed3d5458",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
