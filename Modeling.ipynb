{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96164879-2312-4e92-b1fe-4baa18c65b43",
   "metadata": {},
   "source": [
    "## Essential Libraries for Data Science and Machine Learning\n",
    "\n",
    "Essential Python libraries is imported, each tailored for specific roles within data science and machine learning workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "f664bedc-7391-48de-abdd-97556e9ef149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "67f447eb-b2e5-4603-8834-3cedfd1bac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = ['MSFT', 'UPS']\n",
    "start_date = '2014-03-15'\n",
    "end_date = '2023-03-15'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9a605a-c141-4c87-adee-3a2b63c8c1a8",
   "metadata": {},
   "source": [
    "## Step 1: Loading Financial Data\n",
    "\n",
    "The Read_Data function serves as a step for loading financial data into a Python analysis environment. It automates the retrieval of historical data for a specified financial instrument, identified by its ticker, over a given date range from a CSV file. By constructing a file path based on the ticker symbol, start date, and end date, it locates the correct CSV file within a predetermined directory (Dfs). The data is then loaded into a pandas DataFrame, providing a structured format suitable for subsequent data analysis and manipulation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "335dd939-bfa0-4976-a403-e5272dc8523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_Data(ticker, start_date, end_date):\n",
    "\n",
    "    \"\"\"\n",
    "    Reads data from a CSV file into a pandas DataFrame.\n",
    "    \n",
    "    This function constructs a file path from a specified ticker symbol, start date, and end date,\n",
    "    then reads the corresponding CSV file from the 'Dfs' folder within the current working directory.\n",
    "    \n",
    "    Parameters:\n",
    "    - ticker (str): The ticker symbol for the data file.\n",
    "    - start_date (str): The start date of the data range.\n",
    "    - end_date (str): The end date of the data range.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing the data from the CSV file.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Name of the folder where your CSV file is located\n",
    "    folder_name = 'Dfs'\n",
    "    \n",
    "    # Construct the file name using the provided parameters.\n",
    "    file_name = f\"{ticker}_{start_date}_{end_date}.csv\"\n",
    "    \n",
    "    # Retrieve the current working directory to build the full file path.\n",
    "    current_directory = os.getcwd()\n",
    "    \n",
    "    # Construct the full path to the CSV file\n",
    "    file_path = os.path.join(current_directory, folder_name, file_name)\n",
    "    \n",
    "    # Read and return the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926d41c1-4610-42bf-91ee-b400e68a9f3b",
   "metadata": {},
   "source": [
    "## Step 2: Preparing Data for Analysis\n",
    "\n",
    "The Finalize_df function is a step for preparing the loaded financial data for further analysis. It focuses on cleaning the data by performing two main tasks: removing unnecessary columns related to target predictions that do not match the specified analysis interval, and eliminating any rows that contain missing values (NaN). This process ensures the integrity and relevance of the data. Additionally, it identifies the column that will be used for target predictions, aligning with the chosen interval, making the data set ready for predictive modeling or any subsequent analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "1e675ebb-3811-4818-a375-754209ae15c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Finalize_df(df, ticker, start_date, end_date, interval):\n",
    "\n",
    "    \"\"\"\n",
    "    Cleans the DataFrame by dropping specific columns and rows with NaN values, and identifies the target column.\n",
    "    \n",
    "    This function iterates through the DataFrame's columns to remove any that contain the word 'Target' \n",
    "    but do not match the specified interval. It then identifies the remaining 'Target' column and \n",
    "    removes any rows in the DataFrame that contain NaN values, ensuring data integrity for further analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame to be cleaned and finalized.\n",
    "    - interval (int): The interval value used to identify the relevant 'Target' column to retain.\n",
    "    \n",
    "    Returns:\n",
    "    - tuple: A tuple containing:\n",
    "        - The name of the target column (str).\n",
    "        - The cleaned DataFrame (pd.DataFrame) with irrelevant target columns removed and no NaN values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Identify columns to drop: those including 'Target' in their name but not matching the specified interval.\n",
    "    columns_to_drop = [col for col in df.columns if 'Target' in col and 'Target_'+str(interval) != col]\n",
    "    \n",
    "    # Drop identified columns from the DataFrame.\n",
    "    dft = df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    # Extract the name of the remaining 'Target' column.\n",
    "    Target = [col for col in dft.columns if 'Target' in col][0]\n",
    "\n",
    "    # Get the last row of the DataFrame\n",
    "    last_row = dft.tail(1)\n",
    "    del last_row[Target]\n",
    "    try:\n",
    "        # Save the last row to a CSV file for future predictions\n",
    "        last_row.to_csv(f\"Last_Rows/last_row_of_{ticker}_{start_date}_{end_date}.csv\", index=False)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        # Handle the case where the 'Last_Rows' directory does not exist.\n",
    "        print(\"Error: The specified directory 'Last_Rows' does not exist.\")\n",
    "    \n",
    "    # Remove rows with any NaN values across all columns to ensure data completeness.\n",
    "    df_cleaned = dft.dropna()\n",
    "\n",
    "    return Target, df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefba193-bd56-4e93-99a4-1ea944440a30",
   "metadata": {},
   "source": [
    "## Step 3: Segmenting Data for Model Training and Testing\n",
    "\n",
    "The Split_train_test function is crucial for dividing the cleaned dataset into appropriate subsets for training and testing machine learning models. It first segregates the dataset into features (independent variables) and the target (dependent variable) based on the column names. It excludes non-feature columns like 'Date' and the specified target column from the features set. Utilizing a predefined proportion (e.g., 80% training, 20% testing) and a fixed random state to ensure the splits are reproducible, it then divides these datasets into training and testing sets. This step is necessary for evaluating the performance of predictive models in a controlled and consistent manner, helping to ensure that the model can generalize well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "7cd60e91-2127-4559-aea9-2b286ff9f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Split_train_test(df, Target):\n",
    "\n",
    "    \"\"\"\n",
    "    Splits the DataFrame into training and testing datasets for features and target.\n",
    "\n",
    "    The function separates the DataFrame into features (X) and target (y) datasets by dropping \n",
    "    the specified target column and any non-feature columns (e.g., 'Date'). It then splits these \n",
    "    datasets into training and testing sets using a predefined test size and random state to ensure \n",
    "    reproducibility.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame containing the dataset to be split.\n",
    "    - Target (str): The name of the target column in the DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    - tuple: Contains the training and testing sets for both features (X_train, X_test) and \n",
    "      target (y_train, y_test).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare the features dataset by dropping the target column and other non-numeric/non-feature columns.\n",
    "    X = df.drop([Target, 'Date'], axis=1)\n",
    "    # Extract the target column as the target dataset.\n",
    "    y = df[Target]\n",
    "    \n",
    "     # Split the data into training and testing sets, allocating 20% of the data to the testing set and\n",
    "    # setting a random state for reproducibility.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587d0a6a-9615-41af-b792-32a0cad3f732",
   "metadata": {},
   "source": [
    "## Step 4: Building and Evaluating\n",
    "### The Linear Regression Model\n",
    "\n",
    "The Model_with_LR function marks a essential step in the project: the construction and evaluation of a Linear Regression model. After preparing the data, this function leverages the training set to train a Linear Regression model, applying it thereafter to the testing set to forecast outcomes. The function assesses the model's accuracy and efficiency by computing two key metrics: the Mean Squared Error (MSE) and the R-squared (R²) score. These metrics provide insights into the model's predictive power and the variance it explains, respectively, thereby enabling a quantitative evaluation of the model's performance in predicting the target variable based on the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "74790ede-414b-4a27-8551-502f990be6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_with_LR(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    \"\"\"\n",
    "    Initializes, fits, and evaluates a Linear Regression model using the provided training and testing datasets.\n",
    "    \n",
    "    This function creates a Linear Regression model, fits it to the training data, and then uses the model\n",
    "    to make predictions on the testing set. It evaluates the performance of the model by calculating the\n",
    "    mean squared error (MSE) and the R-squared (R2) score between the actual and predicted values for the target.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): The training dataset containing the features.\n",
    "    - X_test (pd.DataFrame): The testing dataset containing the features.\n",
    "    - y_train (pd.Series): The training dataset containing the target variable.\n",
    "    - y_test (pd.Series): The testing dataset containing the target variable.\n",
    "    \n",
    "    Returns:\n",
    "    - tuple: Contains the fitted Linear Regression model, the MSE, and the R2 score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the Linear Regression model.\n",
    "    LR = LinearRegression()\n",
    "    # Fit the model to the training data.\n",
    "    LR.fit(X_train, y_train)\n",
    "    \n",
    "    # Use the fitted model to make predictions on the test dataset.\n",
    "    y_pred = LR.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model's performance by calculating the Mean Squared Error and R-squared score.\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    return LR, mse, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c264e45c-85cc-4f85-9766-2bb6d3fbabe9",
   "metadata": {},
   "source": [
    "### The OLS Regression Model\n",
    "\n",
    "The Model_with_OLS function implements an Ordinary Least Squares (OLS) regression analysis, following the preparation and segmentation of the dataset. It adds a constant to the feature sets to include an intercept in the model, fits the OLS model to the training data, and then assesses its performance on the test data. The evaluation is carried out through the mean squared error (MSE) and R-squared (R²) score, offering insights into the model's predictive accuracy and the proportion of variance in the dependent variable explained by the independent variables. This step is crucial for understanding how well the linear relationship has been modeled and for identifying potential areas for model improvement or refinement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "07ba123b-5f59-47e0-9779-5ce92e5706ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_with_OLS(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    \"\"\"\n",
    "    Initializes, fits, and evaluates an Ordinary Least Squares (OLS) regression model.\n",
    "    \n",
    "    This function extends the X_train and X_test datasets with a constant term to account for the intercept \n",
    "    in the OLS model, fits the model to the training data, and evaluates its performance on the testing set \n",
    "    by calculating both the mean squared error (MSE) and R-squared (R2) score.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): Training dataset containing the features.\n",
    "    - X_test (pd.DataFrame): Testing dataset containing the features.\n",
    "    - y_train (pd.Series): Training dataset containing the target variable.\n",
    "    - y_test (pd.Series): Testing dataset containing the target variable.\n",
    "    \n",
    "    Returns:\n",
    "    - tuple: Contains the fitted OLS model, the MSE, and the R2 score of the model on the testing set.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extend the features matrix of the training set with a column for the constant term.\n",
    "    X_with_const = sm.add_constant(X_train)\n",
    "    \n",
    "    # Fit the OLS (Ordinary Least Squares) model\n",
    "    ols = sm.OLS(y_train, X_with_const).fit()\n",
    "    \n",
    "    # Extend the features matrix of the testing set with a column for the constant term.\n",
    "    X_test_with_const = sm.add_constant(X_test)\n",
    "    # Use the fitted model to make predictions on the extended testing set.\n",
    "    y_pred = ols.predict(X_test_with_const)\n",
    "    \n",
    "    # Evaluate the performance of the model on the testing set.\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    return ols, mse, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a93116-475e-45e1-b43a-a1e589206459",
   "metadata": {},
   "source": [
    "### The XGBoost Regression Model\n",
    "\n",
    "The Model_with_XGB function deploys an XGBoost regression model to the project. After preparing the dataset, this function configures an XGBoost model with pre-defined hyperparameters, trains it on the dataset, and evaluates its predictive performance using the test data. The function focuses on understanding the model's accuracy through mean squared error (MSE) and R-squared (R²) metrics, crucial for assessing the model's effectiveness in capturing the complexity of the data and making accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "af0ea614-d25b-427c-bc6e-5be8bc04c602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_with_XGB(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    \"\"\"\n",
    "    Initializes, fits, and evaluates an XGBoost regression model.\n",
    "    \n",
    "    This function creates an XGBoost regression model with specified hyperparameters, fits it to the \n",
    "    training data, and uses it to make predictions on the testing set. It evaluates the model's performance\n",
    "    by calculating the mean squared error (MSE) and R-squared (R2) score between the actual and predicted\n",
    "    values for the target variable.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): The training dataset containing the features.\n",
    "    - X_test (pd.DataFrame): The testing dataset containing the features.\n",
    "    - y_train (pd.Series): The training dataset containing the target variable.\n",
    "    - y_test (pd.Series): The testing dataset containing the target variable.\n",
    "    \n",
    "    Returns:\n",
    "    - tuple: Contains the fitted XGBoost model, the MSE, and the R2 score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the XGBoost regressor model with specified hyperparameters.\n",
    "    xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "\n",
    "    # Fit the XGBoost model to the training data.\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Use the trained model to make predictions on the test dataset.\n",
    "    xgb_pred = xgb_model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model's performance on the test data using Mean Squared Error and R-squared metrics.\n",
    "    xgb_mse = mean_squared_error(y_test, xgb_pred)\n",
    "    xgb_r2 = r2_score(y_test, xgb_pred)\n",
    "    \n",
    "    return xgb_model, xgb_mse, xgb_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61237219-16a5-4f08-9145-abe92c0f1bbc",
   "metadata": {},
   "source": [
    "### The Random Forest Regression Model\n",
    "\n",
    "The Model_with_RF function incorporates the use of a Random Forest regression model into the project, marking a move towards leveraging ensemble learning techniques for enhanced predictive modeling. By initiating a Random Forest model with its default settings, the function trains the model on the dataset prepared in previous steps and assesses its predictive power on the test set. The evaluation focuses on the mean squared error (MSE) and R-squared (R²) metrics to gauge the model's accuracy and the proportion of variance it explains in the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "f00e94d2-34ea-4e00-ba1d-010f58b31a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_with_RF(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    \"\"\"\n",
    "    Initializes, fits, and evaluates a Random Forest regression model.\n",
    "    \n",
    "    This function creates a Random Forest regression model with default hyperparameters, fits it to the\n",
    "    training data, and uses it to make predictions on the testing set. It evaluates the model's performance\n",
    "    by calculating the mean squared error (MSE) and R-squared (R2) score between the actual and predicted\n",
    "    values for the target variable.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): The training dataset containing the features.\n",
    "    - X_test (pd.DataFrame): The testing dataset containing the features.\n",
    "    - y_train (pd.Series): The training dataset containing the target variable.\n",
    "    - y_test (pd.Series): The testing dataset containing the target variable.\n",
    "    \n",
    "    Returns:\n",
    "    - tuple: Contains the fitted Random Forest model, the MSE, and the R2 score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize and fit the Random Forest regressor model to the training data.\n",
    "    rf_model = RandomForestRegressor()\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Use the trained model to make predictions on the test dataset.\n",
    "    y_pred_rf = rf_model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model's performance on the test data using Mean Squared Error and R-squared metrics.\n",
    "    mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "    r2_rf = r2_score(y_test, y_pred_rf)\n",
    "    \n",
    "    return rf_model, mse_rf, r2_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e16a7e-89ed-4b6b-9ce4-eab7a77ae534",
   "metadata": {},
   "source": [
    "## Step 5: Selecting and Saving the Optimal Model\n",
    "\n",
    "The Save_Best_Model function integrates model comparison and selection, targeting the identification of the most effective regression model for the dataset based on its predictive performance. This function systematically evaluates Linear Regression, OLS, XGBoost, and Random Forest models, all previously prepared and assessed, to determine the best model according to mean squared error (MSE) and R-squared (R²) metrics. Priority is given to minimizing MSE while using R² as a secondary criterion. Only models surpassing a high threshold of predictive accuracy (R² >= 0.90) are considered for final selection. The best-performing model, if meeting the set criteria, is then serialized (pickled) for persistence, facilitating its reuse without retraining. This step encapsulates the project's goal of optimizing predictive accuracy while providing a mechanism for efficiently deploying the chosen model in future applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "19b53b21-97d6-4986-b8f1-e6bb1a27bf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Save_Best_Model(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    \"\"\"\n",
    "    Evaluates multiple regression models on the given dataset, identifies the best performing model based on\n",
    "    mean squared error (MSE) and R-squared (R2) metrics, and pickles the best model if its R2 score is above 0.90.\n",
    "    \n",
    "    The function considers Linear Regression, OLS, XGBoost, and Random Forest models, comparing their performance\n",
    "    on the given training and testing datasets. The best model is determined by the lowest MSE, with a tiebreaker\n",
    "    based on the highest R2 score.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): The training dataset containing the features.\n",
    "    - X_test (pd.DataFrame): The testing dataset containing the features.\n",
    "    - y_train (pd.Series): The training dataset containing the target variable.\n",
    "    - y_test (pd.Series): The testing dataset containing the target variable.\n",
    "    \n",
    "    Returns:\n",
    "    - The tuple of the best model, its MSE, and R2 score, if its R2 score is above 0.90; otherwise, None.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize variables to store the best model and its metrics\n",
    "    best_model = None\n",
    "    best_mse = float('inf')  # Initialize with a large value\n",
    "    best_r2 = -float('inf')   # Initialize with a small value\n",
    "\n",
    "    # Generate models using predefined functions.\n",
    "    Model_LR = Model_with_LR(X_train, X_test, y_train, y_test)\n",
    "    Model_OLS = Model_with_OLS(X_train, X_test, y_train, y_test)\n",
    "    Model_XGB = Model_with_XGB(X_train, X_test, y_train, y_test)\n",
    "    Model_RF = Model_with_RF(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # List of models to evaluate\n",
    "    models = {\n",
    "        \"Linear Regression\": Model_LR,\n",
    "        \"OLS\": Model_OLS,\n",
    "        \"XGBoost\": Model_XGB,\n",
    "        \"Random Forest\": Model_RF\n",
    "    }\n",
    "\n",
    "    # Evaluate each model, updating the best model based on MSE and R2.\n",
    "    for model_name, (model, mse, r2) in models.items():\n",
    "\n",
    "        # Check if the current model has better performance than the best model found so far\n",
    "        if mse < best_mse:\n",
    "            best_model = model_name\n",
    "            best_mse = mse\n",
    "            best_r2 = r2\n",
    "\n",
    "    # Conditionally choose the best model if its performance is sufficiently high.\n",
    "    if best_r2>=0.90:\n",
    "        \n",
    "        if best_model == \"Linear Regression\":\n",
    "            model_to_pickle = Model_LR\n",
    "        elif best_model == \"OLS\":\n",
    "            model_to_pickle = Model_OLS\n",
    "        elif best_model == \"XGBoost\":\n",
    "            model_to_pickle = Model_XGB\n",
    "        elif best_model == \"Random Forest\":\n",
    "            model_to_pickle = Model_RF\n",
    "\n",
    "    else:\n",
    "        print('Could not create model with high coverage (R^2 >= 0.9) for the given data. Consider changing input parameters.')\n",
    "        \n",
    "    return model_to_pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d2115b-dbef-49e7-ac0d-fd2944b1f808",
   "metadata": {},
   "source": [
    "## Step 6: Persisting the Best Model for Future Use\n",
    "\n",
    "The Make_Pickle function encapsulates the final step in the predictive modeling process: saving the optimal model for future application. By serializing the chosen model into a binary file using Python's pickle module, this function ensures that the model can be easily and efficiently reloaded for subsequent predictions, without the need to retrain. The naming convention for the output file incorporates both the ticker symbol and the interval for which the model was developed, facilitating straightforward identification and retrieval of the model. This step is crucial for operationalizing the model in practical financial analysis scenarios, allowing for quick deployment in predictive tasks related to the specified financial instrument and analysis interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "6987879f-de7d-439e-abc4-57baf92e74f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_Pickle(model_to_pickle, interval, ticker, start_date, end_date):\n",
    "\n",
    "    \"\"\"\n",
    "    Pickles and saves the provided model to a file, named according to the specified ticker and interval.\n",
    "    \n",
    "    The function writes the model to a binary file within the 'Pickles' directory. The naming convention for the file\n",
    "    includes the model's associated ticker symbol and interval, ensuring easy identification and retrieval of the model\n",
    "    for future predictions or analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    - model_to_pickle: The model object to be pickled and saved.\n",
    "    - interval (str or int): The interval associated with the model, contributing to the file's name.\n",
    "    - ticker (str): The ticker symbol associated with the model, contributing to the file's name.\n",
    "    \n",
    "    Outputs:\n",
    "    - On success, the model is saved to a .pkl file in the 'Pickles' directory, and a confirmation message is printed.\n",
    "    - On failure, an error message is printed detailing the nature of the problem encountered.\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If the 'Pickles' directory does not exist.\n",
    "    - IOError: If there is an issue writing to the file.\n",
    "    - pickle.PicklingError: If the model object cannot be serialized.\n",
    "    - Exception: Catches any other unexpected errors that occur during execution.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Open the file and pickle the model object to the file.\n",
    "    try:\n",
    "        with open(f\"Pickles/best_model_{ticker}_{interval}_{start_date}_{end_date}.pkl\", \"wb\") as f:\n",
    "            # Serialize and save the 'model_to_pickle' object into the file.\n",
    "            pickle.dump(model_to_pickle, f)\n",
    "        # Print a success message indicating the model has been successfully pickled and saved.    \n",
    "        print(f\"Model pickled successfully for {ticker}_{interval}.\")\n",
    "    except FileNotFoundError:\n",
    "        # Handle the case where the 'Pickles' directory does not exist.\n",
    "        print(\"Error: The specified directory 'Pickles' does not exist.\")\n",
    "    except IOError:\n",
    "        # Handle general input/output errors, such as failure in file writing.\n",
    "        print(\"IOError: There was an issue writing to the file.\")\n",
    "    except pickle.PicklingError:\n",
    "        # Handle errors specific to the pickling process, such as if the object cannot be serialized.\n",
    "        print(\"PicklingError: There was an issue pickling the model.\")\n",
    "    except Exception as e:\n",
    "        # Catch any other unexpected exceptions and print an error message detailing the problem.\n",
    "        print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97387ae5-fa67-47ca-a4ff-79e103064ff0",
   "metadata": {},
   "source": [
    "## Step 7: Automating Model Development and Persistence Across Time Intervals\n",
    "\n",
    "The Make_Predictions function has a comprehensive process tailored for financial data analysis, specifically aimed at generating and saving predictive models for a range of time intervals. This automated pipeline initiates with reading historical stock data for a specified ticker within a given date range. It then iteratively cleans the data, selects the best model based on predefined performance metrics (mean squared error and R-squared score), and finally serializes the optimal model for each interval, making it readily accessible for future predictions.\n",
    "\n",
    "For each interval in the predetermined list, the function executes several key steps: data preparation (cleaning and finalizing), dataset partitioning into training and testing sets, model comparison and selection, and model serialization. This end-to-end process not only enhances efficiency in model development but also ensures that the models are tailored to specific intervals, thereby improving the relevance and accuracy of predictions for different time frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "34112051-96f5-48f0-9148-9b4050005d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_Predictions(ticker, start_date, end_date):\n",
    "\n",
    "    \"\"\"\n",
    "    Processes stock data for a given ticker and date range to train, evaluate, and save the best predictive model\n",
    "    for various time intervals.\n",
    "    \n",
    "    For each specified interval, this function reads historical stock data, cleans it, splits it into training and testing\n",
    "    sets, identifies the best model based on performance metrics, and pickles this model for future use.\n",
    "    \n",
    "    Parameters:\n",
    "    - ticker (str): The stock ticker symbol for which data is processed.\n",
    "    - start_date (str): The start date for the data range to be processed.\n",
    "    - end_date (str): The end date for the data range to be processed.\n",
    "    \n",
    "    Processes:\n",
    "    - For each interval in a predefined list, performs data cleaning, model training and evaluation,\n",
    "      and pickles the best-performing model.\n",
    "    \n",
    "    Outputs:\n",
    "    - Saves pickled models for each interval to the disk.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the list of intervals to generate models for.\n",
    "    List_of_intervals = [1, 2, 7, 14, 28]\n",
    "\n",
    "    # Read the data for the specified ticker and date range.\n",
    "    df = Read_Data(ticker, start_date, end_date)\n",
    "    print(ticker,'\\'s Data frame has been built Successfully!')\n",
    "    \n",
    "    # Iterate over each interval, process the data, and save the best model.\n",
    "    for interval in List_of_intervals: \n",
    "        # Prepare the data for the given interval.\n",
    "        Target, df_cleaned = Finalize_df(df, ticker, start_date, end_date, interval)\n",
    "        # Split the data into training and testing sets.\n",
    "        X_train, X_test, y_train, y_test = Split_train_test(df_cleaned, Target)\n",
    "        # Identify and save the best model for the interval.\n",
    "        model_to_pickle = Save_Best_Model(X_train, X_test, y_train, y_test)\n",
    "        # Check if a model was selected to be pickled before attempting to pickle it.\n",
    "        if model_to_pickle:\n",
    "            print(f'Best model selected for, {interval}, with MSE = {best_mse} and Rsquared = {best_r2}')\n",
    "            Make_Pickle(model_to_pickle, interval, ticker, start_date, end_date)\n",
    "        else:\n",
    "            print(f\"No suitable model found for interval {interval} to pickle.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "b35ed2a9-4d4e-4533-9050-d152027542c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pickled successfully for MSFT_1.\n",
      "Model pickled successfully for MSFT_2.\n",
      "Model pickled successfully for MSFT_7.\n",
      "Model pickled successfully for MSFT_14.\n",
      "Model pickled successfully for MSFT_28.\n",
      "Model pickled successfully for UPS_1.\n",
      "Model pickled successfully for UPS_2.\n",
      "Model pickled successfully for UPS_7.\n",
      "Model pickled successfully for UPS_14.\n",
      "Model pickled successfully for UPS_28.\n"
     ]
    }
   ],
   "source": [
    "for t in ticker:\n",
    "    Make_Predictions(t, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6b8148-d9ee-4d5f-9fce-a65c688996ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdd8c6f-5f9c-4d21-9d4e-bdde25c7cf33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
